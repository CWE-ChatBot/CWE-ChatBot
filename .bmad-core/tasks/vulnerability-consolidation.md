# vulnerability-consolidation

This task provides detailed instructions for consolidating security findings from multiple sub-agents into a unified vulnerability assessment. Use this task as a standalone consolidation workflow or as a referenced component within larger security analysis tasks.

## Prerequisites

- Multiple sub-agent analysis results available (Security-Reviewer, Dependency-Scanner, Pattern-Analyzer, Test-Validator)
- Access to vulnerability assessment templates
- Understanding of risk scoring methodology and cross-validation principles

## Consolidation Methodology

### Step 1: Data Collection and Parsing

**1.1 Sub-Agent Output Inventory**
Create an inventory of all available sub-agent analysis results:

```yaml
sub_agent_results:
  security_reviewer:
    status: "[completed/partial/failed]"
    findings_count: "[number]"
    analysis_scope: "[files/modules analyzed]"
    output_format: "[structured/unstructured]"
    
  dependency_scanner:
    status: "[completed/partial/failed]"
    dependencies_scanned: "[number]"
    vulnerabilities_found: "[number]"
    cve_mappings: "[available/unavailable]"
    
  pattern_analyzer:
    status: "[completed/partial/failed]"
    patterns_checked: "[number]"
    violations_found: "[number]"
    framework_coverage: "[percentage]"
    
  test_validator:
    status: "[completed/partial/failed]"
    test_coverage: "[percentage]"
    security_tests: "[count]"
    gaps_identified: "[number]"
```

**1.2 Raw Finding Extraction**
For each sub-agent with successful completion, extract raw findings using these parsing guidelines:

**Security-Reviewer Parsing**:
- **SAST Findings**: Look for vulnerability types (SQL injection, XSS, etc.), file locations, severity ratings
- **LLM Business Logic**: Extract business logic flaws, authorization issues, data handling problems
- **Combined Analysis**: Identify findings that combine SAST detection with LLM context analysis

**Dependency-Scanner Parsing**:
- **CVE Database Matches**: Extract CVE numbers, affected packages, severity scores (CVSS)
- **Supply Chain Risks**: Identify dependency update recommendations, license compliance issues
- **Transitive Dependencies**: Map indirect dependency vulnerabilities to root packages

**Pattern-Analyzer Parsing**:
- **Secure Coding Violations**: Extract pattern rule violations, framework-specific issues
- **Anti-Pattern Detection**: Identify problematic coding patterns, security misconfigurations
- **Best Practice Gaps**: Document deviations from secure coding standards

**Test-Validator Parsing**:
- **Coverage Gaps**: Identify untested security scenarios, missing test types
- **Test Quality Issues**: Extract ineffective tests, incomplete security validation
- **Security Test Recommendations**: Document needed security test enhancements

### Step 2: Normalization Process

**2.1 Standardized Vulnerability Format**
Transform each raw finding into this normalized structure:

```yaml
normalized_vulnerability:
  # Identification
  id: "[AUTO-GENERATE: SAST-001, BIZ-002, DEP-003, PATTERN-004, TEST-005]"
  source_finding_id: "[original ID from sub-agent if available]"
  
  # Classification
  type: "[sql-injection, auth-bypass, vulnerable-dependency, insecure-pattern, test-gap]"
  category: "[OWASP-A01, OWASP-A02, etc. or CWE-89, CWE-79, etc.]"
  severity: "[critical/high/medium/low]"
  
  # Source Attribution
  source: "[semgrep-sast, llm-analysis, dependency-scan, pattern-analysis, test-analysis]"
  source_sub_agent: "[Security-Reviewer, Dependency-Scanner, Pattern-Analyzer, Test-Validator]"
  detection_method: "[automated-scan, manual-analysis, hybrid-detection]"
  
  # Location and Context
  location: "[file:line for code, package==version for dependencies, test-file for test gaps]"
  affected_components: "[list of affected modules, functions, or services]"
  code_context: "[relevant code snippet or configuration if applicable]"
  
  # Vulnerability Details
  description: "[Clear, business-focused vulnerability description]"
  technical_details: "[Technical specifics for developers]"
  attack_vector: "[How this vulnerability can be exploited]"
  prerequisites: "[Conditions required for exploitation]"
  
  # Risk Assessment
  cvss_score: "[CVSS score if available]"
  cvss_vector: "[CVSS vector string]"
  business_impact: "[Specific business consequences]"
  likelihood: "[high/medium/low exploitation likelihood]"
  exploitability: "[simple/moderate/complex]"
  
  # External References
  cve: "[CVE number if from dependency scan, null for code issues]"
  cwe: "[CWE classification if applicable]"
  external_references: "[links to advisories, documentation]"
  
  # Remediation
  remediation: "[Specific, actionable fix guidance]"
  remediation_effort: "[hours/days estimate]"
  remediation_priority: "[immediate/short-term/medium-term/long-term]"
  responsible_team: "[team responsible for implementing fix]"
  
  # Metadata
  confidence_score: "[initial confidence before cross-validation]"
  discovered_date: "[when this vulnerability was identified]"
  last_updated: "[when this record was last modified]"
```

**2.2 Severity Harmonization**
Apply consistent severity ratings across all findings:

**Critical Severity Criteria**:
- Allows remote code execution without authentication
- Bypasses all authentication/authorization controls
- Exposes all customer data or financial information
- CVE with CVSS score 9.0-10.0
- Business logic flaws affecting core business operations

**High Severity Criteria**:
- Allows privilege escalation or data exposure
- Bypasses significant security controls
- Affects sensitive data handling
- CVE with CVSS score 7.0-8.9
- Business logic flaws affecting user accounts or payments

**Medium Severity Criteria**:
- Allows information disclosure or denial of service
- Partial security control bypass
- CVE with CVSS score 4.0-6.9
- Pattern violations that increase attack surface

**Low Severity Criteria**:
- Information disclosure with limited impact
- Security misconfigurations with minimal exposure
- CVE with CVSS score 0.1-3.9
- Minor pattern violations or test coverage gaps

### Step 3: De-duplication Algorithm

**3.1 Duplicate Detection Rules**
Apply these rules to identify duplicate findings across sub-agents:

**Exact Duplicates** (merge completely):
- Same vulnerability type + same file location + same line number
- Same CVE number from different dependency scanners
- Same pattern violation detected by multiple tools

**Related Duplicates** (merge with enhanced context):
- Same vulnerability type in related functions/modules
- Same root cause manifesting in different locations
- Dependency vulnerability affecting multiple components

**Similar Issues** (group but keep separate):
- Same vulnerability class but different instances
- Related patterns across different code sections
- Test gaps in similar functionality areas

**3.2 Merging Process**
When merging duplicate findings:

1. **Use Highest Severity**: Take the most severe rating among duplicates
2. **Combine Descriptions**: Merge technical details and context from all sources
3. **Aggregate Sources**: List all sub-agents that detected the issue
4. **Enhance Remediation**: Combine fix guidance from multiple perspectives
5. **Update Confidence**: Increase confidence score for cross-validated findings

### Step 4: Cross-Validation and Confidence Scoring

**4.1 Cross-Validation Analysis**
For each normalized vulnerability, determine validation status:

**High Confidence** (Cross-validated by 2+ sub-agents):
- Same vulnerability detected by different methods (SAST + LLM)
- Dependency issue confirmed by multiple scanning tools  
- Pattern violation detected by both automated and manual analysis
- Test gap identified by multiple assessment approaches

**Medium Confidence** (Single detection but strong indicators):
- Single sub-agent detection but critical severity
- Strong business logic context supporting the finding
- CVE with public exploits available
- Pattern violation with clear security implications

**Low Confidence** (Single detection with uncertainty):
- Single sub-agent detection with low/medium severity
- Limited context or business impact
- Potential false positive indicators
- Tool-specific detection that may not apply to actual deployment

**4.2 Confidence Score Calculation**
Calculate numerical confidence scores (0-100):

```
Base Confidence:
- Multiple sub-agent detection: +40 points
- Single sub-agent detection: +20 points

Severity Bonus:
- Critical severity: +30 points
- High severity: +20 points  
- Medium severity: +10 points
- Low severity: +5 points

Validation Bonus:
- Has CVE reference: +10 points
- Strong business context: +10 points
- Clear attack vector: +10 points
- Public exploits available: +5 points

False Positive Penalties:
- Tool-specific limitation: -10 points
- Unclear business impact: -5 points
- No clear remediation: -5 points
```

### Step 5: Risk Prioritization Framework

**5.1 Overall Risk Score Calculation**
Calculate weighted risk scores for prioritization:

```
Risk Score = (Severity Points × Confidence Multiplier × Business Impact Multiplier)

Severity Points:
- Critical: 4 points
- High: 3 points
- Medium: 2 points  
- Low: 1 point

Confidence Multiplier:
- High Confidence (80-100): 1.2x
- Medium Confidence (50-79): 1.0x
- Low Confidence (0-49): 0.8x

Business Impact Multiplier:
- Financial systems: 1.5x
- Customer data: 1.3x
- Core business logic: 1.2x
- Administrative functions: 1.0x
- Development/testing: 0.8x
```

**5.2 Prioritization Categories**
Group vulnerabilities into remediation priorities:

**Priority 1 - Immediate (0-7 days)**:
- Critical severity with high confidence
- Any vulnerability with active exploits
- Authentication/authorization bypasses
- Financial or payment system vulnerabilities

**Priority 2 - Short-term (1-4 weeks)**:
- High severity with medium+ confidence
- Critical severity with low confidence (needs validation)
- Data exposure vulnerabilities
- Cross-validated medium severity issues

**Priority 3 - Medium-term (1-3 months)**:
- Medium severity with high confidence
- High severity with very low confidence
- Pattern violations affecting security architecture
- Significant test coverage gaps

**Priority 4 - Long-term (3+ months)**:
- Low severity issues
- Process improvements
- Minor pattern violations
- Non-critical test coverage improvements

### Step 6: Quality Assurance and Validation

**6.1 Consolidation Quality Checks**
Perform these validation checks on the consolidated results:

**Completeness Validation**:
- Are all sub-agent findings accounted for?
- Are severity distributions reasonable?
- Do confidence scores align with evidence?
- Are remediation guidelines actionable?

**Consistency Validation**:
- Are similar vulnerabilities rated consistently?
- Do cross-validated findings have appropriate confidence boosts?
- Are business impact assessments aligned with organizational priorities?
- Do timeline estimates match severity and complexity?

**Accuracy Validation**:
- Do technical details match the vulnerability descriptions?
- Are CVE references correct and current?
- Do remediation approaches address root causes?
- Are affected components correctly identified?

**6.2 False Positive Review**
Review low-confidence findings for potential false positives:

**False Positive Indicators**:
- Single tool detection with no business context
- Tool limitation or misconfiguration suspected
- Finding contradicts known system behavior
- No clear exploitation path available

**False Positive Handling**:
- Flag for manual review rather than automatic exclusion
- Document reasons for potential false positive classification
- Provide recommendation for additional validation
- Include in appendix with appropriate disclaimers

## Output Deliverables

### Consolidated Vulnerability Dataset
The consolidation process produces:

1. **Normalized Vulnerability List**: All findings in standardized format
2. **Cross-Validation Analysis**: Confidence scoring and validation results
3. **Risk Prioritization**: Categorized remediation priorities with timelines
4. **Quality Metrics**: Consolidation statistics and confidence indicators
5. **Exclusion Documentation**: Rationale for any filtered findings

### Integration with Security Reports
This consolidated dataset integrates directly with:
- `security-consolidated-report-tmpl.yaml` template variables
- Executive summary statistics and metrics
- Detailed vulnerability breakdown sections
- Remediation roadmap planning
- NIST SSDF compliance assessment

### Consolidation Audit Trail
Maintain complete traceability:
- Original sub-agent finding references
- Consolidation decisions and rationale
- Confidence score calculations
- Risk prioritization methodology applied
- Quality assurance validation results

This vulnerability consolidation task ensures consistent, high-quality integration of multi-sub-agent security analysis results into actionable, prioritized vulnerability assessments suitable for executive reporting and development team remediation planning.