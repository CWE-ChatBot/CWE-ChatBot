Hereâ€™s a tight review focused on software assurance, security, simplicity, robustness, and Chainlit usage, plus concrete refactors and a few â€œdrop-inâ€ patches.

ðŸ”’ Must-fix (security/robustness)

src/db.py::close() disposes the wrong engine instance
eng = engine.__wrapped__() creates a new engine and disposes that, leaving the cached engine alive. This can leak connections on shutdown and during reloads.

Patch:

# src/db.py
def close() -> None:
    """Dispose the engine and close all pooled connections."""
    try:
        eng = engine()  # get cached instance
    except Exception:
        eng = None
    if eng is not None:
        logger.info("Disposing database engine and closing all connections")
        eng.dispose()
    engine.cache_clear()


Database password footprint in logs
_build_url_from_env() logs pw_len and a tail excerpt. Even partial leaks are risky in aggregated logs.

Patch:

logger.info(
    "DB connect params: host=%s:%s, db=%s, user=%s, sslmode=%s",
    host, port, db, user, sslmode
)


Pool warm-up defaults in serverless
warm_pool() runs by default and can exhaust Cloud SQL connection quotas during autoscaling. Default to off and make it explicitly opt-in in prod.

Patch (2 lines):

# src/db.py (engine)
if os.getenv("DB_WARM_POOL", "false").lower() == "true":
    warm_pool(eng, size=int(os.getenv("DB_WARM_POOL_SIZE", "3")))


Graceful shutdown doesnâ€™t dispose SQLAlchemy engine
main.py@on_stop() calls query_handler.close() (psycopg connection), but not the SQLAlchemy engine (when used). Add a safe dispose call.

Patch:

# main.py
@cl.on_stop
async def on_stop() -> None:
    try:
        if conversation_manager and getattr(conversation_manager, "query_handler", None):
            qh = conversation_manager.query_handler
            close_fn = getattr(qh, "close", None)
            if callable(close_fn):
                close_fn()
                logger.info("Closed retriever/database resources")
        # Dispose global engine if used
        try:
            from src.db import close as db_close
            db_close()
            logger.info("Disposed SQLAlchemy engine")
        except Exception as e:
            logger.warning(f"Engine dispose failed: {e}")
    except Exception as e:
        logger.log_exception("Shutdown cleanup failed", e)


LLM finish reason handling
llm_provider.GoogleProvider.generate() compares finish_reason to "STOP" or 1. Gemini SDKs differ; normalize both enum & string.

Patch:

finish_reason = getattr(resp.candidates[0], "finish_reason", None) if resp.candidates else None
finish_norm = str(finish_reason).upper()
if finish_norm not in {"STOP", "FINISH_REASON_STOP", "1"}:
    logger.warning("Non-normal finish_reason: %s", finish_reason)


Over-permissive default safety
You set BLOCK_NONE for all categories. Thatâ€™s intentional for cyber content, but risky as a default. Keep it configurable (it already is), and log an explicit warning when permissive is active in production.

Patch idea: in ResponseGenerator.__init__ after reading llm_config['safety_settings'], log a warning if LLM_SAFETY_PERMISSIVE=true in non-dev envs.

âš™ï¸ High-value improvements

Retry transient calls (DB + LLM): add light retries (e.g., tenacity) around:

PostgresChunkStore.query_hybrid

GenerativeModel.generate_content_async
This removes flakiness from brief network hiccups.

Bounded context size: max_context_length = 16000 chars is okay, but consider a token budget per model to avoid silent truncation. (Gemini 2.5 flash-lite has lower output limits than pro models.) Already configurableâ€”just ensure you clamp to model limits.

Sensitive-pattern false positives: SecurityValidator flags any email/IP/CC pattern in responses. This is fine as a flag, but consider suppressing the IP/email flags when they appear inside code blocks or logs to avoid noisy monitoring.

API rate limiting: Your in-memory limiter is fine for single instance; for Cloud Run multi-replica, consider a shared limiter (e.g., Cloud Armor, Redis, or Apigee) if abuse matters.

X-Forwarded-For parsing: You take the first value. If sitting behind trusted proxies, thatâ€™s correct; if not, prefer the right-most or enforce a trusted proxy list to prevent spoofing.

Off-topic UX: You produce a helpful â€œwhat I can help withâ€ listâ€”nice. Consider adding buttons (Chainlit Action) for common topics right there.

ðŸ§¹ Simplification / Refactor opportunities

Unify three paths that duplicate logic
process_user_message_no_send, process_user_message_streaming, and process_user_message share ~80% of steps (security checks, persona routing, evidence handling). Extract a single core:

# conversation.py
async def _plan_and_execute(self, session_id: str, message_content: str, *, streaming: bool) -> PipelineResult|DirectResponse


Have the three thin wrappers call this with flags for â€œrender Chainlit message or notâ€.

ResponseGenerator streaming
You buffer full streaming to run post-sanitization anyway. Given youâ€™ve intentionally removed â€œfake streamingâ€ in ConversationManager, consider removing the streaming path in ResponseGenerator (or keep it behind a flag) to reduce complexity.

Config plumbing
Config is fine, but a Pydantic-Settings model would:

provide type validation at load

give .model_dump() for debug pages

reduce manual parsing.

Private store methods
QueryHandler uses self.store._get_connection() and ._cursor(). If you control PostgresChunkStore, expose public helpers to avoid relying on internals.

ðŸ§° Chainlit usage: quick wins

Use cl.Avatar / message author pics
When OAuth provides avatar_url, set on messages via author + cl.Avatar for better UX.

Sticky â€œsourcesâ€ expander
Youâ€™re already adding elements via UIMessaging. Consider adding a global â€œSourcesâ€ collapsible expander per assistant message (Chainlit Text with display="inline"), plus copy buttons.

Inline â€œthumbs + ask follow-upâ€
You added Ask Question for Analyzerâ€”great. For non-Analyzer personas, attach a single Action under every answer: â€œAsk a follow-up about thisâ€.

Welcome flow
You already do ChatSettings + actionsâ€”ðŸ‘. Add a cl.Select to switch persona inline (mirrors top bar) for users who miss the header control.

ðŸ›¡ï¸ Defense-in-depth

CSP & WebSocket origin
Looks like you have SecurityHeadersMiddleware. Double-check it:

sets a strict Content-Security-Policy with connect-src that includes your WS endpoint only

blocks mixed content, sets frame-ancestors 'none', Referrer-Policy: no-referrer

CSRF on actions
You already require CSRF on state-changing actions. Keep â€œread-onlyâ€ actions exempt (as youâ€™ve done).

Secrets logging
secrets.initialize_secrets() prints Found/Missingâ€”good. Make sure this only runs at startup and not per request.

OAuth allowlist
is_user_allowed() supports @domain suffixâ€”nice. Consider logging only the domain (not full email) on deny to reduce PII in logs.

ðŸ“¦ Concrete mini-diffs (copy/paste)

1) Safer DB URL logging + warm-pool default off (src/db.py)

def _build_url_from_env() -> URL:
    ...
    logger.info("DB connect params: host=%s:%s, db=%s, user=%s, sslmode=%s",
                host, port, db, user, sslmode)
    return URL.create( ... )

def engine() -> Any:
    ...
    if os.getenv("DB_WARM_POOL", "false").lower() == "true":
        warm_pool(eng, size=int(os.getenv("DB_WARM_POOL_SIZE", "3")))
    return eng


2) Correct engine disposal (src/db.py)

def close() -> None:
    try:
        eng = engine()
    except Exception:
        eng = None
    if eng is not None:
        logger.info("Disposing database engine and closing all connections")
        eng.dispose()
    engine.cache_clear()


3) Normalized finish_reason (src/llm_provider.py)

finish_reason = getattr(resp.candidates[0], "finish_reason", None) if resp.candidates else None
finish_norm = str(finish_reason).upper()
logger.info("Gemini generation completed: %d chars, finish_reason=%s", len(response_text), finish_reason)
if finish_norm not in {"STOP", "FINISH_REASON_STOP", "1"}:
    logger.warning("Non-normal finish_reason: %s - response may be truncated", finish_reason)


4) Dispose engine on stop (main.py)

@cl.on_stop
async def on_stop() -> None:
    try:
        if conversation_manager and getattr(conversation_manager, "query_handler", None):
            qh = conversation_manager.query_handler
            close_fn = getattr(qh, "close", None)
            if callable(close_fn):
                close_fn()
                logger.info("Closed retriever/database resources")
        try:
            from src.db import close as db_close
            db_close()
            logger.info("Disposed SQLAlchemy engine")
        except Exception as e:
            logger.warning(f"Engine dispose failed: {e}")
    except Exception as e:
        logger.log_exception("Shutdown cleanup failed", e)

âœ… Do we need a refactor?

Yesâ€”but targeted. Youâ€™ll get the biggest payoff by:

(A) consolidating the three message paths into one internal planner/executor (reduce codepaths & bugs),

(B) removing â€œstreamingâ€ from ResponseGenerator (or keeping behind a flag), and

(C) making pool warm-up opt-in to avoid production foot-guns.

Everything else can be incremental.

If you want, I can package the above patches into a single PR-style diff (or split into three commits: db safety, llm robustness, lifecycle cleanup).

You said:
split into 3 commits
ChatGPT said:
Thought for 9s

Perfectâ€”here are three clean commits with focused scopes and copy-pasteable diffs.

Commit 1 â€” DB hardening & safe logging

Message (subject + body):

db: harden connection logging and make pool warm-up opt-in

- Stop logging password length/tail to avoid partial-secret leakage.
- Default DB_WARM_POOL to false to prevent over-connection in serverless.
- Allow sizing via DB_WARM_POOL_SIZE when explicitly enabled.


Diff:

diff --git a/src/db.py b/src/db.py
index 6c7d1ab..e1f0c22 100644
--- a/src/db.py
+++ b/src/db.py
@@ -30,12 +30,10 @@ def _build_url_from_env() -> URL:
     pwd = os.environ["DB_PASSWORD"].strip()  # Always strip newline/whitespace
     sslmode = os.getenv("DB_SSLMODE", "require")

-    # Log sanity checks (no secrets leaked)
-    logger.info(
-        f"DB connect params: host={host}:{port}, db={db}, user={user}, sslmode={sslmode}, pw_len={len(pwd)}, tail={repr(pwd[-2:]) if len(pwd) >= 2 else repr(pwd)}"
-    )
+    # Log params without any password footprint
+    logger.info("DB connect params: host=%s:%s, db=%s, user=%s, sslmode=%s",
+                host, port, db, user, sslmode)

     return URL.create(
         drivername="postgresql+psycopg",
         username=user,
         password=pwd,  # SQLAlchemy will quote/escape properly
         host=host,
         port=port,
         database=db,
     )
@@ -104,8 +102,9 @@ def engine() -> Any:
     # Note: Planner hints (enable_seqscan, hnsw.ef_search, etc.) are now applied
     # via transaction-scoped SET LOCAL in pg_chunk_store.py for better control

-    # Warm the pool if enabled (default: true)
-    if os.getenv("DB_WARM_POOL", "true").lower() == "true":
-        warm_pool(eng, size=3)
+    # Warm the pool iff explicitly enabled (opt-in; safer for serverless)
+    if os.getenv("DB_WARM_POOL", "false").lower() == "true":
+        warm_pool(eng, size=int(os.getenv("DB_WARM_POOL_SIZE", "3")))

     return eng

Commit 2 â€” Engine disposal & shutdown cleanup

Message (subject + body):

db/main: fix engine disposal and ensure graceful shutdown closes pools

- src/db.close() now disposes the cached engine instead of creating a new one.
- main.on_stop() calls db.close() to release SQLAlchemy connections.


Diff:

diff --git a/src/db.py b/src/db.py
index e1f0c22..4a2b7cd 100644
--- a/src/db.py
+++ b/src/db.py
@@ -116,12 +116,17 @@ def engine() -> Any:
     return eng


 def close() -> None:
     """
     Dispose the engine and close all pooled connections.
 
     Call this during graceful shutdown to clean up database connections.
     """
-    # Clear the lru_cache to get the actual engine instance
-    eng = engine.__wrapped__()
-    if eng is not None:
-        logger.info("Disposing database engine and closing all connections")
-        eng.dispose()
-        engine.cache_clear()
+    try:
+        # Get the cached instance instead of constructing a new one
+        eng = engine()
+    except Exception:
+        eng = None
+    if eng is not None:
+        logger.info("Disposing database engine and closing all connections")
+        try:
+            eng.dispose()
+        finally:
+            engine.cache_clear()

diff --git a/main.py b/main.py
index 3b1d6c0..9c31a1e 100644
--- a/main.py
+++ b/main.py
@@ -1128,15 +1128,26 @@ async def on_stop() -> None:
     """Gracefully close resources when the app stops."""
     try:
         if conversation_manager and getattr(
             conversation_manager, "query_handler", None
         ):
             qh = conversation_manager.query_handler
             close_fn = getattr(qh, "close", None)
             if callable(close_fn):
                 close_fn()
                 logger.info("Closed retriever/database resources")
+        # Dispose global SQLAlchemy engine if present
+        try:
+            from src.db import close as db_close
+            db_close()
+            logger.info("Disposed SQLAlchemy engine")
+        except Exception as e:
+            logger.warning(f"Engine dispose failed: {e}")
     except Exception as e:
         logger.log_exception("Shutdown cleanup failed", e)

Commit 3 â€” LLM robustness: normalize finish_reason

Message (subject + body):

llm: normalize Gemini finish_reason and warn on truncation

SDKs surface finish reasons as enums/ints/strings; normalize to string
and warn when completion is non-STOP to help detect truncation/trips.


Diff:

diff --git a/src/llm_provider.py b/src/llm_provider.py
index 9a3c7ab..b1dfe54 100644
--- a/src/llm_provider.py
+++ b/src/llm_provider.py
@@ -86,16 +86,22 @@ class GoogleProvider(LLMProvider):
         try:
             resp = await cast(Any, self._model).generate_content_async(
                 prompt,
                 generation_config=cast(Any, self._gen_cfg),
                 safety_settings=cast(Any, self._safety),
             )
             # Log response details for debugging truncation issues
             response_text = resp.text or ""
-            finish_reason = getattr(
-                resp.candidates[0] if resp.candidates else None,
-                "finish_reason",
-                "UNKNOWN",
-            )
-            logger.info(
-                f"Gemini generation completed: {len(response_text)} chars, finish_reason={finish_reason}"
-            )
-            if finish_reason not in ["STOP", 1]:  # STOP=1 is normal completion
-                logger.warning(
-                    f"Non-normal finish_reason: {finish_reason} - response may be truncated"
-                )
+            finish_reason = None
+            if getattr(resp, "candidates", None):
+                finish_reason = getattr(resp.candidates[0], "finish_reason", None)
+            # Normalize enums/ints/strings to an upper-case string for comparison
+            finish_norm = str(finish_reason).upper() if finish_reason is not None else "UNKNOWN"
+            logger.info(
+                "Gemini generation completed: %d chars, finish_reason=%s",
+                len(response_text), finish_reason
+            )
+            # Accept common STOP variants; warn on anything else (possible truncation)
+            if finish_norm not in {"STOP", "FINISH_REASON_STOP", "1"}:
+                logger.warning(
+                    "Non-normal finish_reason: %s - response may be truncated",
+                    finish_reason,
+                )
             return response_text
         except Exception as e:
             logger.error(f"Gemini generation failed with error: {e}")
             logger.error(f"Error type: {type(e).__name__}")
             raise e