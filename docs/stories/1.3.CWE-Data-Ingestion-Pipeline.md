# Story 1.3: Initial CWE Data Ingestion Pipeline

**Status**: Approved

## Story

**As a** data engineer,
**I want** an automated pipeline to ingest a small, curated subset of CWE data (e.g., 5-10 specific CWEs from MITRE's XML/JSON) into a vector database,
**so that** the chatbot can begin to retrieve basic information.

## Acceptance Criteria

1.  A Python script or service is developed to download the latest public CWE XML/JSON data from the MITRE website.
2.  The script can parse and extract relevant information (ID, Name, Description, Relationships) for a small, pre-defined subset of CWEs (e.g., CWE-79, CWE-89, CWE-123).
3.  Embeddings are generated for this subset of CWEs using a selected embedding model (e.g., a local sentence transformer or an external API).
4.  The generated embeddings and corresponding CWE metadata (ID, Name) are successfully stored in the chosen vector database (e.g., Pinecone, Weaviate, or a simple in-memory vector store for MVP validation).
5.  The ingestion process is repeatable and can be manually triggered via a command-line interface or simple function call, and produces a verifiable local output (e.g., confirmation log, sample data file, or queryable local vector store).

## Security Requirements

1.  **Secure Data Handling:** The XML/JSON parser used MUST be configured to prevent parsing-based attacks, such as XML External Entity (XXE) injection, by disabling DTDs and external entity resolution.
2.  **API Key Security:** If an external service is used for generating embeddings, the API key MUST be loaded from environment variables or a secure secrets manager. It MUST NOT be hardcoded in the source code.
3.  **Data Integrity:** The download script SHOULD validate the integrity of the downloaded CWE data if checksums or signatures are provided by the source.

## Tasks / Subtasks

-   [ ] **Task 1: Develop Data Download Module** (AC: 1, Security: 3)
    -   [ ] Write a Python function to download the CWE data file from the official MITRE URL.
    -   [ ] Implement error handling for network issues or file not found.
-   [ ] **Task 2: Implement Secure Parser** (AC: 2, Security: 1)
    -   [ ] Choose a Python library for parsing (e.g., `lxml` for XML).
    -   [ ] Configure the parser securely to disable DTD processing.
    -   [ ] Write functions to extract the required fields for a hardcoded list of 5-10 CWEs.
-   [ ] **Task 3: Integrate Embedding Model** (AC: 3, Security: 2)
    -   [ ] Select and integrate an embedding model (recommend a local Sentence Transformer model to start, to align with self-hosting goals).
    -   [ ] Write a function that takes the extracted text and generates vector embeddings.
    -   [ ] Ensure any required API keys are handled securely via environment variables.
-   [ ] **Task 4: Implement Vector Database Storage** (AC: 4)
    -   [ ] Set up a local vector database instance (e.g., ChromaDB or FAISS) for initial development.
    -   [ ] Write a function to connect to the database and store the CWE metadata and its corresponding embedding.
-   [ ] **Task 5: Create CLI Trigger** (AC: 5)
    -   [ ] Use a library like `argparse` or `click` to create a simple command-line interface.
    -   [ ] Create a main script that orchestrates the download, parse, embed, and store steps.
    -   [ ] Add logging to show progress and confirm successful ingestion.

## Dev Notes

* **RAG Foundation:** This story is the first step in building our Retrieval-Augmented Generation (RAG) system. The quality of the data parsing and embedding will directly impact the chatbot's accuracy.
* **Data Source:** The official CWE data can be found at the [MITRE CWE website](https://cwe.mitre.org/data/downloads.html). We should target the comprehensive XML or JSON formats.
* **Embedding Model Choice:** For the MVP and to align with the self-hosting requirement (`FR19`), using a local open-source model from a library like `sentence-transformers` is highly recommended. This avoids sending potentially sensitive query data to external APIs.
* **Vector Database Choice:** To simplify local development, we can start with an in-memory or file-based vector store like **FAISS** or **ChromaDB**. The architecture should allow for swapping this with a managed cloud service later.

## Testing

### Unit Tests

-   [ ] Write a test for the parser to ensure it correctly extracts data from a sample XML/JSON snippet.
-   [ ] Write a test for the embedding function to confirm it produces vectors of the expected dimension.

### Integration Tests

-   [ ] Write an integration test for the entire pipeline that uses a small, local sample of the CWE data file, runs the ingestion process, and verifies that the data is correctly stored in a temporary local vector database.

### Security Verification

-   [ ] **Code Review:** Manually review the source code to confirm that no API keys are hardcoded.
-   [ ] **Parser Configuration:** Verify that the XML/JSON parsing library is explicitly configured to prevent XXE and other parsing-related vulnerabilities.

### Manual Verification

-   [ ] Run the ingestion script from the command line.
-   [ ] Check the script's log output for success messages.
-   [ ] Write a separate, simple query script to connect to the local vector database and retrieve one of the ingested CWEs to confirm it was stored correctly.

## Change Log

| Date          | Version | Description                   | Author      |
|---------------|---------|-------------------------------|-------------|
| July 30, 2025 | 1.0     | Initial story creation from PRD | John (PM)   |