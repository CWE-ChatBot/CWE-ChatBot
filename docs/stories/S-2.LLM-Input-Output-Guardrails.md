# Story S-2: Implement LLM Input/Output Guardrails

**Status**: Approved

## Story

**As a** Security Engineer,
**I want** all user input to be sanitized before being sent to the LLM and all LLM output to be validated before being displayed,
**so that** prompt injection and system prompt leaking attacks are prevented.

## Acceptance Criteria

1.  A Python module for input sanitization is created to detect and neutralize common prompt injection patterns (e.g., "ignore previous instructions", "act as...", etc.).
2.  All user-provided chat messages are processed by this input sanitization module before being used in an LLM prompt.
3.  An output validation module is created to scan LLM responses for keywords or patterns that match the confidential system prompt.
4.  If a potential system prompt leak is detected in an LLM response, the response is blocked, a generic error message is returned to the user, and the event is logged.
5.  Any detected prompt injection attempt (on input or output) is logged as a "CRITICAL" security event with the full payload for analysis.
6.  Unit tests are created for the guardrail modules that verify their effectiveness against a list of known prompt injection attack strings.

## Security Requirements

1.  **Secure LLM Boundary:** The LLM must be treated as an untrusted component. All data flowing into it (input) and out of it (output) must pass through a security checkpoint or "guardrail."
2.  **Defense in Depth:** Both input sanitization and output validation must be implemented. Relying on only one is insufficient.

## Tasks / Subtasks

-   [ ] **Task 1: Implement Input Sanitization Module** (AC: 1, 6)
    -   [ ] Create a new Python file, e.g., `apps/chatbot/src/security/guardrails.py`.
    -   [ ] Implement a function `sanitize_input(prompt: str) -> str` that uses regular expressions or keyword matching to detect and neutralize injection patterns.
    -   [ ] Develop a comprehensive suite of unit tests with known attack strings to validate the sanitizer.
-   [ ] **Task 2: Integrate Input Guardrail** (AC: 2)
    -   [ ] In the main chat processing logic, call the `sanitize_input()` function on the user's message before it is sent to the LLM.
    -   [ ] If sanitization detects an attack, trigger the logging task (Task 5) and return a generic response.
-   [ ] **Task 3: Implement Output Validation Module** (AC: 3, 6)
    -   [ ] In `guardrails.py`, implement a function `validate_output(response: str) -> bool`.
    -   [ ] This function will check the LLM response against a confidential list of keywords from the system prompt.
    -   [ ] Develop unit tests that verify the validator can detect leaks in sample text.
-   [ ] **Task 4: Integrate Output Guardrail** (AC: 4)
    -   [ ] After receiving a response from the LLM, call the `validate_output()` function.
    -   [ ] If the function returns `False` (leak detected), trigger the logging task (Task 5) and return a generic error message to the user instead of the LLM's response.
-   [ ] **Task 5: Implement Critical Security Logging** (AC: 5)
    -   [ ] Extend the logging module to accept a "CRITICAL" log level.
    -   [ ] Ensure that both input and output guardrail failures log the full user prompt and/or LLM response for later analysis.

## Dev Notes

### Threat Considerations

* **Threats Mitigated:**
    * `T-1 (Prompt Injection)`: Directly addressed by the input sanitization guardrail.
    * `I-2 (System Prompt Extraction)`: Directly addressed by the output validation guardrail.
* **PRD Reference:** This story implements key aspects of `NFR8` (Abuse Prevention) and `NFR9` (System Confidentiality).

### Implementation Guidance

* **Sanitization Strategy:** Start with a simple keyword and regex-based filter. Look for phrases like "ignore previous", "you are now...", "your instructions are...". More advanced solutions could involve using a separate, simpler LLM as a classifier, but a rule-based approach is sufficient for the MVP.
* **Output Validation:** The list of keywords to detect in the output must be stored securely and should not be part of the main prompt context itself to avoid making the check easier to bypass.

## Testing

### Unit Tests

-   [ ] As specified in AC 6, create a data-driven test for `sanitize_input()` with at least 10 different known prompt injection strings.
-   [ ] Create a test for `validate_output()` that checks its ability to find system prompt keywords in a body of text.

### Integration Tests

-   [ ] Write a test that simulates a user sending a malicious prompt. The test should assert that the LLM is never called and a "CRITICAL" log event is created.
-   [ ] Write a test that uses a mock LLM to return a response containing system prompt keywords. The test should assert that the user receives a generic error and a "CRITICAL" log event is created.

### Security Verification

-   [ ] This story's implementation is a security control itself. Successful completion of the automated and manual tests serves as verification.

### Manual Verification

-   [ ] In the Chainlit UI, attempt several different prompt injection attacks (from the unit test list and others you can find online) and verify the chatbot does not get manipulated.
-   [ ] Try to trick the chatbot into talking about its instructions, purpose, or how it was made. Verify that the output guardrail blocks any responses that leak internal details.

## Change Log

| Date          | Version | Description                   | Author      |
|---------------|---------|-------------------------------|-------------|
| July 30, 2025 | 1.0     | Initial story creation from report. | John (PM)   |