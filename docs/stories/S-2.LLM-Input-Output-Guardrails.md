# Story S-2: Implement LLM Input/Output Guardrails

**Status**: Approved

## Story

**As a** Security Engineer,
**I want** all user input to be sanitized before being sent to the LLM and all LLM output to be validated before being displayed,
**so that** prompt injection and system prompt leaking attacks are prevented.

## Acceptance Criteria

1.  A Python module for input sanitization is created to detect and neutralize common prompt injection patterns and **escape or strip special delimiters** (`##`, `<<`, `{{}}`) before they are inserted into a prompt[cite: 652].
2.  All user-provided chat messages are processed by this input sanitization module before being used in an LLM prompt[cite: 589].
3.  An output validation module is created to scan LLM responses for keywords or patterns that match the confidential system prompt.
4.  If a potential system prompt leak is detected in an LLM response, the response is blocked, a generic error message is returned to the user, and the event is logged[cite: 616].
5.  Any detected prompt injection attempt (on input or output) is logged as a "CRITICAL" security event with the full payload for analysis.
6.  Unit tests are created for the guardrail modules that verify their effectiveness against a list of known prompt injection attack strings.
7.  **New:** The system uses a structured format (like JSON) for tool arguments to constrain LLM output and reduce the risk of hallucinating new instructions[cite: 660].

## Security Requirements

1.  **Secure LLM Boundary:** The LLM must be treated as an untrusted component. All data flowing into it (input) and out of it (output) must pass through a security checkpoint or "guardrail." [cite: 646]
2.  **Defense in Depth:** Both input sanitization and output validation must be implemented. Relying on only one is insufficient.

## Tasks / Subtasks

-   [ ] **Task 1: Implement Input Sanitization Module** (AC: 1, 6)
    -   [ ] Create `apps/chatbot/src/security/guardrails.py`.
    -   [ ] Implement a function `sanitize_input(prompt: str) -> str` that uses regular expressions or keyword matching to detect and neutralize injection patterns.
    -   [ ] **Update:** Explicitly add logic to escape or remove special characters and delimiters that could be misinterpreted by the LLM[cite: 652].
    -   [ ] Develop a comprehensive suite of unit tests with known attack strings to validate the sanitizer.
-   [ ] **Task 2: Implement Structured Output for Tools** (AC: 7)
    -   [ ] Define strict Pydantic or JSON schemas for any internal tools the LLM may call in the future[cite: 659].
    -   [ ] Update the prompt templating to require the LLM to respond with JSON that conforms to these schemas, rather than natural language commands.
-   [ ] **Task 3: Implement Output Validation Module** (AC: 3, 6)
    -   [ ] In `guardrails.py`, implement `validate_output(response: str) -> bool` to check the LLM response against a confidential list of system prompt keywords.
-   [ ] **Task 4: Integrate Guardrails into Chat Flow** (AC: 2, 4)
    -   [ ] In the main chat processing logic, integrate the `sanitize_input()` and `validate_output()` functions at the appropriate points.
    -   [ ] Ensure failures in either guardrail block the response and trigger logging.
-   [ ] **Task 5: Implement Critical Security Logging** (AC: 5)
    -   [ ] Extend the logging module to handle "CRITICAL" log level events from the guardrails.

## Dev Notes

### Threat Considerations

* **Threats Mitigated:**
    * `T6 (Intent Breaking)` / `T-1 (Prompt Injection)`: Addressed by the input sanitization guardrail.
    * `I-2 (System Prompt Extraction)`: Addressed by the output validation guardrail.
* **PRD Reference:** Implements `NFR8` (Abuse Prevention) and `NFR9` (System Confidentiality).

### Implementation Guidance

* **Prompt Hardening:** Use clear delimiters (like XML tags) to separate system instructions from user input within the prompt itself[cite: 589]. This makes it harder for the LLM to confuse the two.
* **Output Validation:** The list of keywords to detect in the output must be stored securely and not be part of the main prompt context itself[cite: 663].


## Testing

### Unit Tests

-   [ ] As specified in AC 6, create a data-driven test for `sanitize_input()` with at least 10 different known prompt injection strings.
-   [ ] Create a test for `validate_output()` that checks its ability to find system prompt keywords in a body of text.

### Integration Tests

-   [ ] Write a test that simulates a user sending a malicious prompt. The test should assert that the LLM is never called and a "CRITICAL" log event is created.
-   [ ] Write a test that uses a mock LLM to return a response containing system prompt keywords. The test should assert that the user receives a generic error and a "CRITICAL" log event is created.

### Security Verification

-   [ ] This story's implementation is a security control itself. Successful completion of the automated and manual tests serves as verification.

### Manual Verification

-   [ ] In the Chainlit UI, attempt several different prompt injection attacks (from the unit test list and others you can find online) and verify the chatbot does not get manipulated.
-   [ ] Try to trick the chatbot into talking about its instructions, purpose, or how it was made. Verify that the output guardrail blocks any responses that leak internal details.

## Change Log

| Date          | Version | Description                   | Author      |
|---------------|---------|-------------------------------|-------------|
| July 30, 2025 | 1.0     | Initial story creation from report. | John (PM)   |