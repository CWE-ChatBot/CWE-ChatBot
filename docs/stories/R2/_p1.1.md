What to change (concretely)

Make CWEQueryHandler retrieval-only. It should return ranked chunks and nothing else.

Introduce a stateless ProcessingPipeline. Given the query, ranked chunks, and user context, it aggregates by CWE, scores, explains, filters, and suggests improvements—returning a typed QueryResult.

Keep ConversationManager as the orchestrator. It wires Retriever → Pipeline → ResponseGenerator (streaming), and owns cancellation/timeouts and telemetry.

Use DI for pipeline components. Pass in ConfidenceCalculator, CWEFilter, ExplanationBuilder, QuerySuggester so they’re mockable.

Minimal interfaces (lean & testable)
# models.py
from dataclasses import dataclass
from typing import Any, Dict, List, Iterable

@dataclass
class Chunk:
    doc_id: str
    text: str
    score: float
    cwe_id: str
    meta: Dict[str, Any]

@dataclass
class Recommendation:
    cwe_id: str
    confidence: float
    explanation: str
    supporting_chunks: List[Chunk]

@dataclass
class QueryResult:
    query: str
    recommendations: List[Recommendation]
    suggestions: List[str]  # query improvements, follow-ups

# query_handler.py
class CWEQueryHandler:
    def __init__(self, retriever, *, boost_conf: Dict[str, float] | None = None):
        self.retriever = retriever
        self.boost_conf = boost_conf or {}

    async def process_query(self, query: str, user_context: Dict[str, Any]) -> List[Chunk]:
        chunks = await self.retriever.search(query, user_context)
        # apply boosting/force-inject only (no app logic)
        return chunks

# processing/pipeline.py
class ProcessingPipeline:
    def __init__(self, confidence, cwe_filter, explainer, suggester, *, cap:int=10):
        self.confidence = confidence
        self.cwe_filter = cwe_filter
        self.explainer = explainer
        self.suggester = suggester
        self.cap = cap

    def generate_recommendations(self, query: str, chunks: List[Chunk], user_ctx: Dict) -> QueryResult:
        # 1) group by CWE
        by_cwe: Dict[str, List[Chunk]] = {}
        for ch in chunks:
            by_cwe.setdefault(ch.cwe_id, []).append(ch)

        # 2) score/confidence
        recs: List[Recommendation] = []
        for cwe_id, group in by_cwe.items():
            conf = self.confidence.compute(group, query=query, user_ctx=user_ctx)
            if not self.cwe_filter.allow(cwe_id, conf, user_ctx):
                continue
            expl = self.explainer.build(cwe_id, group, conf, query, user_ctx)
            recs.append(Recommendation(cwe_id, conf, expl, group[:3]))

        # 3) sort & cap
        recs.sort(key=lambda r: r.confidence, reverse=True)
        recs = recs[: self.cap]

        # 4) suggestions
        suggestions = self.suggester.propose(query, chunks, recs, user_ctx)

        return QueryResult(query=query, recommendations=recs, suggestions=suggestions)

# conversation.py (or manager)
class ConversationManager:
    def __init__(self, query_handler, pipeline, response_generator, clock, logger):
        self.query_handler = query_handler
        self.pipeline = pipeline
        self.response_generator = response_generator
        self.clock = clock
        self.logger = logger

    async def process_user_message_streaming(self, query: str, user_ctx: Dict):
        chunks = await self.query_handler.process_query(query, user_ctx)
        result = self.pipeline.generate_recommendations(query, chunks, user_ctx)

        async for token in self.response_generator.generate_response_streaming(
            query=query,
            recommendations=result.recommendations,
            suggestions=result.suggestions,
            user_ctx=user_ctx,
        ):
            yield token

Async/streaming & lifecycle notes

Keep the retriever async, but the pipeline can be sync (CPU-bound). If any step calls an LLM, expose an async variant or do it in ResponseGenerator.

Propagate cancellation/timeouts from ConversationManager to retriever and generator.

Pipeline should be stateless; store session/user prefs in your Chainlit UserContext, not inside components.

Cross-cutting concerns

Config/Feature flags: thresholds, caps, boosting → injectable config for A/B.

Telemetry: record timings per step, counts per CWE, filter drop reasons.

Caching: cache retrieval results by (query, persona, corpus_version) to speed retries.

Determinism: if you use randomness (e.g., tie-breakers), seed it to stabilize tests.

Testing & rollout

Unit-test each component (ConfidenceCalculator, CWEFilter, ExplanationBuilder) with small fixtures.

Golden tests for ProcessingPipeline.generate_recommendations.

Contract test: given fixed chunks, both old and new paths produce equivalent QueryResult (within tolerance).

Ship behind a flag (USE_NEW_PIPELINE) and compare metrics before removing process_query_with_recommendations.