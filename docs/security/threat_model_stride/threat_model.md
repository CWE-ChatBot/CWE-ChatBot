## Threat Model

| Threat Type | Scenario | Potential Impact |
|-------------|----------|------------------|
| Spoofing | An attacker could perform a DNS spoofing or Man-in-the-Middle (MITM) attack against the CWE Data Ingestion Service's connection to cwe.mitre.org. This would allow the attacker to provide a malicious, altered CWE corpus, poisoning the Vector Database with false or dangerous information that would then be served to all users. | The chatbot's core knowledge base would be compromised, leading it to provide dangerously incorrect security advice, misclassify vulnerabilities, or even recommend malicious code snippets to users. This undermines the fundamental purpose and trustworthiness of the application. |
| Spoofing | A user leverages the 'Bring Your Own LLM' (BYO LLM) feature to configure a malicious endpoint they control. This endpoint could be designed to mimic a legitimate LLM API but always returns harmful or biased information, or it could be used to probe the application's internal request structure by analyzing the prompts it receives. | The attacker can control the LLM responses for their own account, potentially using the chatbot's trusted UI to generate and exfiltrate malicious content. It could also lead to information disclosure about the application's internal prompt engineering techniques. |
| Spoofing | During the OAuth 2.0 authentication flow, if the application does not strictly validate the 'redirect_uri' parameter, an attacker could trick a user into clicking a crafted link. This could cause the authorization code to be sent to an attacker-controlled server, allowing the attacker to exchange it for an access token and spoof the victim's identity. | The attacker gains full access to the victim's account, including their chat history, personal configurations (role, preferences), and the ability to interact with the chatbot as the victim user. |
| Tampering | A user submits a crafted query designed to manipulate the RAG process. For example, they inject control characters or prompt fragments into their query that, when combined with the retrieved CWE data, alter the final prompt sent to the LLM, causing it to ignore its instructions or reveal its underlying system prompt. | This could lead to prompt injection, bypassing the intended 'Guardrails' (LlamaFirewall) and causing the chatbot to generate unintended, inappropriate, or malicious responses. It could also expose the proprietary system prompt, which is valuable intellectual property. |
| Tampering | An attacker with access to modify the IaC (Terraform) files in the monorepo could alter the configuration of the Cloud Run service. They could change environment variables to point to a malicious database, disable security headers, or weaken the CORS policy, making the deployed application vulnerable. | Widespread compromise of the application's security posture, potentially leading to data breaches, client-side attacks (XSS), or redirection of data to attacker-controlled systems. The integrity of the entire deployment is compromised. |
| Tampering | An attacker gains access to the PostgreSQL database and modifies a user's `llm_model_config` JSONB field. They could change the `endpoint` to their own malicious server or alter other parameters. The user would be unaware that their subsequent queries are being sent to and processed by an untrusted LLM. | All of the user's queries, which may contain sensitive or confidential information, are exfiltrated to the attacker's server. The user receives responses from a malicious, untrusted model, leading to potential harm. |
| Repudiation | A user with the 'Admin' role makes a malicious change using the `/user/config` API endpoint, such as updating another user's configuration to point to a malicious LLM. The application logs the action but fails to log critical context, such as the source IP address of the admin or the specific 'before' and 'after' state of the changed data. | The malicious admin can deny they performed the action. Without sufficient audit logs, it becomes impossible to prove who made the change, hindering incident response and accountability. |
| Repudiation | The application stores chat history in the `messages` table but does not digitally sign or create an immutable record of the LLM's response. A user receives harmful advice from the chatbot, but a malicious administrator later alters the `content` field in the database to remove the evidence. | The user cannot prove that the application provided harmful advice, and the company cannot reliably audit past conversations for quality or safety issues. This erodes trust and creates legal and liability risks. |
| Repudiation | A user provides feedback on a chatbot response (FR27), but the system only records the feedback itself without linking it cryptographically to the exact message content, the `llm_model_used`, and the `cwe_ids_suggested` at that point in time. If any of these related records are changed, the context of the feedback is lost. | It becomes impossible to reliably trace user feedback back to a specific version of a model's output or a specific version of the CWE data, making it difficult to improve the system or address safety complaints accurately. The user's action of providing feedback can be disputed. |
| Information Disclosure | A user crafts a malicious prompt that causes the LLM to ignore the retrieved CWE context and instead reveal sensitive data from its training set or its system prompt. This is a classic prompt injection or 'jailbreak' attack targeting the LLM directly. | Disclosure of the proprietary system prompt, which may contain instructions, few-shot examples, or other intellectual property. In a worst-case scenario, if the LLM has been fine-tuned on any sensitive data, that data could be revealed. |
| Information Disclosure | The `llm_api_key_id` is stored in the `users` table, and the actual API keys are stored elsewhere. However, if an attacker gains read access to the `users` table (e.g., via SQL injection), they obtain the UUIDs for all stored keys. They can then attempt to use these IDs to access the keys if the key storage system's authorization is weak. | While not a direct key exposure, the disclosure of key identifiers significantly lowers the bar for an attacker to retrieve the actual keys, potentially leading to fraudulent use of paid LLM services and unauthorized data access. |
| Information Disclosure | An error in the application logic, particularly in the NLP/AI Service, causes a detailed technical stack trace to be returned to the user-facing chatbot UI. This trace might contain internal file paths, library versions, configuration variable names, or snippets of source code. | Attackers gain valuable intelligence about the application's internal workings, technology stack, and potential vulnerabilities, which can be used to craft more targeted attacks. |
| Denial of Service | An attacker, knowing the application uses a RAG architecture, submits a series of complex, computationally expensive, or very broad queries. These queries force the Vector Database to perform exhaustive searches and cause the LLM to process large, complex contexts, consuming significant computational resources and hitting API rate limits. | The application becomes slow or unresponsive for legitimate users. The costs associated with the Vector DB and LLM API calls could spike dramatically, leading to financial loss. If rate limits are hit (NFR10), the service becomes unavailable. |
| Denial of Service | The CWE Data Ingestion Pipeline is triggered (e.g., via a webhook or scheduled job) but is fed a malformed or extremely large CWE data file. The parsing logic enters an infinite loop or consumes excessive memory/CPU, causing the Cloud Run instance or Cloud Function to crash repeatedly. | The chatbot's knowledge base cannot be updated, becoming stale and potentially inaccurate over time. If the ingestion service shares resources or has downstream impacts, it could affect the availability of other system components. |
| Denial of Service | The application relies on Redis (Cloud Memorystore) for caching and session management. An attacker sends a large number of requests that create new, unique sessions, or they find a way to repeatedly bust the cache. This flood of requests fills up the Redis memory, causing legitimate cache entries and sessions to be evicted. | Application performance degrades significantly as it can no longer benefit from caching. Legitimate users may be unexpectedly logged out or lose their conversation context. In a severe case, the application may become unstable or crash if it cannot write to a full Redis instance. |
| Elevation of Privilege | A flaw exists in the `/user/config` PUT endpoint. A regular user discovers they can include an 'id' field for another user in the JSON payload of their request. If the backend logic prioritizes the ID from the request body over the authenticated user's ID from the JWT, the user can overwrite another user's configuration. | A regular user could escalate their privileges by overwriting an Admin's configuration, or tamper with any other user's settings, including their role or BYO LLM configuration, leading to account takeover or data exfiltration. |
| Elevation of Privilege | The application defines multiple user roles (e.g., 'Developer', 'Admin') but the enforcement logic is flawed. An authorization check is correctly performed on the main endpoint, but a subsequent internal function call or microservice communication fails to re-verify the user's role, assuming the initial check was sufficient. | A lower-privileged user who is authorized to access a general feature might be able to trigger a sensitive, admin-only action deep within the application's business logic, bypassing the intended RBAC controls. |
| Elevation of Privilege | During the OAuth user creation process, the application correctly fetches the user's email from the ID Provider. However, it also allows the user's role to be set based on a parameter in the initial request URL. An attacker could craft the URL to assign themselves the 'Admin' role upon their first login. | An attacker can self-provision an administrative account, gaining full control over the application and its users without needing to compromise any existing accounts. |


## Improvement Suggestions

- Provide more detail on how the 'llm_api_key_id' is resolved to an actual secret. Is it a direct lookup in Google Secret Manager? Clarifying this flow would help analyze potential vulnerabilities in accessing another user's API key.
- The document mentions 'LlamaFirewall' as the 'Guardrails' component. Please elaborate on what specific types of input/output validation it performs (e.g., checking for hate speech, PII, prompt injection patterns) to better assess its effectiveness against tampering and information disclosure.
- Clarify the trust boundary between the centralized cloud hosting and the self-hosted deployment options. Do they share any APIs or data stores? Understanding this separation is key to modeling threats that could cross from one environment to the other.
- The authentication flow diagram is good, but lacks detail on how the JWT is validated. Specify what claims (e.g., 'iss', 'aud') are checked to ensure the token is from the correct provider and intended for this application, which is crucial for preventing spoofing.
- The data ingestion pipeline pulls data from MITRE. Please describe the transport mechanism (e.g., HTTPS, FTP) and whether the integrity of the downloaded file (e.g., via a checksum/hash provided by MITRE) is verified before processing.
- Describe the data retention and deletion policy (NFR39). How is user data (chat history, PII) permanently removed upon request? This information is critical for analyzing threats related to data remanence and privacy compliance.
- The project uses a monorepo. Please detail the branch protection rules and code review policies for the `main` branch, especially concerning changes to the `infrastructure/` directory, to better assess threats related to tampering with security configurations via IaC.
