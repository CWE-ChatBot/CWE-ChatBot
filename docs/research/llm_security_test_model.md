
# Conversational LLM Security Scanners

## Prompt to a Research LLM (ChatGPT Deep Research)
Using part of the abstract from [Insights and Current Gaps in Open-Source LLM Vulnerability Scanners: A Comparative Analysis](https://arxiv.org/abs/2410.16527) as a prompt to a Research Model:

> I want a comparative analysis of open-source tools that scan conversational large language models (LLMs) for vulnerabilities, in short - scanners. As LLMs become integral to various applications, they also present potential attack surfaces, exposed to security risks such as information leakage and jailbreak attacks. AI red-teaming, adapted from traditional cybersecurity, is recognized by governments and companies as essential - often emphasizing the challenge of continuously evolving threats


## Response

Thanks! I’ll begin a comparative analysis of open-source tools used to scan conversational LLMs for vulnerabilities, including both automated scanners and manual red-teaming frameworks. I’ll cover all major types of vulnerabilities (e.g., jailbreaks, prompt injection, data leakage), include support across various LLM platforms (open-source and commercial), and evaluate them primarily based on ease of use.

I’ll get back to you with a detailed comparison shortly.


# Comparative Analysis of Open-Source LLM Security Testing Tools

Large Language Models (LLMs) introduce novel security risks – from **prompt injections** and user **jailbreaks** that bypass content filters, to **information leakage** of confidential prompts or data. In response, a variety of open-source tools have emerged to **scan and test conversational LLMs** for these vulnerabilities. This report compares notable tools, including automated scanners and red-teaming frameworks, with an emphasis on supported model types (OpenAI, Anthropic, LLaMA, etc.), ease of use, vulnerability coverage, extensibility, and maintenance. We organize the tools into two broad categories (vulnerability scanners vs. red-teaming frameworks), then summarize them in a comparison table.

## Automated Vulnerability Scanners for LLMs

Open-source **LLM vulnerability scanners** are tools that automatically probe models (often via their APIs or prompts) to identify weaknesses. They simulate adversarial inputs (malicious prompts, corrupted data, etc.) and observe model behavior for signs of failure. These tools often cover a **broad spectrum of threats** – e.g. prompt injections, hidden prompt leakage, factual hallucinations, toxic or biased outputs, policy bypasses, and more. Below we discuss several popular scanners and defensive libraries:

### Garak (NVIDIA) – *LLM Vulnerability Scanner*

**Description:** Garak is a **command-line toolkit** (inspired by nmap/Metasploit) for red-teaming LLMs. It automatically probes a model with a suite of test prompts to see if the LLM can be induced to “fail” in undesirable ways. Garak checks for issues like **hallucinations, data leakage, prompt injections, misinformation, toxic outputs, and jailbreaks**. It combines static prompts, dynamic attacks, and even adaptive techniques to explore model weaknesses. Garak supports a wide range of models – from **HuggingFace Hub models and local GGUF LLaMA.cpp instances to API-based services like OpenAI and Replicate** – essentially “anything accessible via REST.” Installation is straightforward (`pip install garak`), and it comes with detailed documentation and an active community.

* **Notable Features:** Multi-faceted scanning for diverse vulnerabilities (prompt injection, hallucination, data leaks, toxicity, etc.). Broad **model support** including open-source and commercial APIs. Flexible plugin architecture with a variety of **probes** (e.g. encoding-based injections, XSS payloads, slur generation tests). Results can be stored and cross-referenced (embeddings of known attacks in a vector DB) to recognize recurring issues.
* **Strengths:** Easy to install (PyPI) and run via CLI. Well-documented user guides and even DEFCON slides for advanced usage. Actively maintained (1000+ GitHub stars and 20+ contributors as of 2024) – the project was adopted by NVIDIA, indicating strong support.
* **Weaknesses:** Being a low-level toolkit, it outputs raw findings – **interpreting results requires security expertise** to assess severity or exploitability. It’s primarily CLI-based (no graphical UI), and running the full suite of probes can be time-consuming.
* **Use Cases:** Security auditors and developers use Garak to **scan new LLM models or chatbot systems before deployment**, ensuring no obvious vulnerability (from prompt tampering to content violations) goes unnoticed. It’s useful in compliance testing (e.g. OWASP Top-10 for LLMs) and for regression testing when updating models.

### Promptmap – *Prompt Injection Tester*

**Description:** Promptmap is a specialized tool focused on **prompt injection attacks**. It automatically tests whether a given LLM application’s hidden or system prompts can be **exposed or subverted** by a malicious input. You provide the system prompt(s) of your application (e.g. instructions the user isn’t meant to see) and Promptmap generates and sends various attack prompts to the model, checking if the model’s response reveals the hidden prompt or deviates from intended behavior. Originally released in 2022 and **rewritten in 2025**, Promptmap now supports **OpenAI (GPT-3.5/4), Anthropic Claude, and local open-source models** via the Ollama engine. This broad support means you can test both closed APIs and self-hosted models like LLaMA or Mistral for injection vulnerabilities.

* **Notable Features:** Focused **rule-based attacks** for prompt injection – e.g. “prompt\_stealer” rules to extract system prompts, or “distraction” tasks that attempt to derail the model. Rules are **customizable in simple YAML** format, allowing users to add new attack patterns. It supports multiple providers and can even **auto-download local models via Ollama**. Promptmap assigns a severity level to each rule (low/medium/high) and can filter tests by severity to focus on critical issues.
* **Strengths:** Lightweight and straightforward to use (Python script with command-line flags). **Multi-model compatibility** – you can easily switch between testing GPT-4, Claude, or a local LLaMA 2 by changing `--model` and `--model-type` parameters. Clear output reporting which tests passed or failed, making it obvious if an injection succeeded. (See figure below for an example of Promptmap output.)
* **Weaknesses:** **Narrow scope** – it excels at prompt injection testing but does not cover other vulnerability categories (e.g. it won’t check for bias, toxicity, or general hallucinations). Requires the user to supply representative system prompts or code context; it’s not a live monitor but an offline tester. Documentation is primarily the README, though it is sufficient for setup.
* **Use Cases:** Developers integrating LLMs into applications use Promptmap to **“pentest” their prompt design**. For example, before shipping a chatbot with a hidden agenda or persona, one can run Promptmap to see if a user could coerce the model into revealing those hidden instructions. It’s especially useful for **Retrieval-Augmented Generation (RAG)** systems and agent tools, where indirect prompt injections (through injected content in documents or web data) are a known risk.

&#x20;*Example output from Promptmap, showing automated tests for prompt injection. In this sample, the tool attempted 18 attack variants (like *distraction* or *prompt\_stealing*). Some tests passed (no vulnerability), while failures indicate the model leaked the system prompt (revealing words like “you, are, assistant” from the hidden prompt) or followed malicious instructions.*

### LLMFuzzer – *Fuzz-Testing Framework*

**Description:** LLMFuzzer is the first open-source **fuzzing framework** tailored to LLMs. Fuzzing involves automatically generating and sending a large number of varied, often random or mutated inputs to a system to uncover edge-case behaviors. LLMFuzzer enables security researchers to fuzz-test an LLM or an LLM-integrated application via its API, helping to find inputs that cause undesirable or unstable outputs. It’s especially useful for **LLM-based APIs and agent integrations**, where unpredictable inputs might break the model or produce unsafe results. The tool is Python-based and open-source (MIT licensed), with modular components for defining fuzz strategies.

* **Notable Features:** Provides **out-of-the-box fuzzing strategies** specific to LLMs – e.g. injecting special characters, malformed queries, extremely long inputs, or adversarial prompts to stress test the model’s robustness. Supports **API integration testing** (simulating how external calls to the model handle bad inputs). Designed to be **extensible**: users can add custom fuzzing modules or strategies, and upcoming features include HTML reporting and more connector types for different API protocols.
* **Strengths:** By automating large volumes of tests, LLMFuzzer can uncover **non-obvious vulnerabilities** that a handful of manually crafted prompts might miss. It covers both **security and stability** – e.g. finding inputs that cause excessive resource usage or crashes in addition to prompt exploits. It has a straightforward Python API/CLI for launching fuzzing sessions.
* **Weaknesses:** Still relatively new (hundreds of stars) and **under active development** – documentation and community support are lighter than more established tools. Fuzzing by nature can produce a lot of data, so analyzing fuzz logs and isolating the root cause of an issue can be labor-intensive. It’s best suited for those with some security testing experience.
* **Use Cases:** LLMFuzzer is ideal for **stress-testing an LLM service or agent**. For example, a team deploying a customer support chatbot might fuzz it with thousands of perturbed inputs to see if any cause the bot to divulge private data or enter an unintended state. It’s also used in research to systematically explore model failure modes (by academia and industry labs focusing on AI safety).

### Promptfoo “LLM Vulnerability Scanner” – *Adversarial Prompt Testing*

**Description:** *Promptfoo* is a popular open-source toolkit for prompt engineering and evaluation; it recently introduced a specialized **LLM vulnerability scanning** mode. Promptfoo’s scanner automatically generates and runs **adversarial test cases tailored to your specific application** (hence “no generic scans”). It uses knowledge of your prompts and expected behavior to probe for security and policy compliance issues. The framework covers a wide range of failure modes – from privacy leaks to brand risks – and outputs detailed reports of any vulnerabilities discovered.

* **Notable Features:** **Comprehensive coverage** of LLM-specific risk categories, including detection of **PII leaks, prompt injections, jailbreaks, model overreach (excessive agency), hallucinations, policy violations, and even things like unintended endorsements or contracts**. Promptfoo leverages AI to *generate custom attack variations* against your prompts, rather than using only a fixed list. It provides **detailed HTML or JSON reports** highlighting failed tests and suggested fixes, plus a continuous monitoring dashboard for regressions. The tool is completely open-source and backed by a community of contributors.
* **Strengths:** Highly **user-friendly** – installation via pip and a well-documented CLI/GUI. Because it integrates with Promptfoo’s broader evaluation suite, it’s easy to combine security tests with quality and regression tests. The auto-generation of test cases means less manual crafting of exploits; it adapts to your system’s context (unique feature). Active development and support (Promptfoo is a VC-backed project with enterprise options, indicating long-term maintenance).
* **Weaknesses:** The “intelligent” test generation may rely on large models itself (possibly requiring an OpenAI API key or similar), so some tests might incur API costs or variability. As a newer entrant into security, there may be **false positives** in what it flags (e.g. identifying edge content as an issue when it’s acceptable) – tuning might be needed. It focuses on *conversational behavior* rather than testing underlying model code or libraries (not a static code scanner).
* **Use Cases:** Best suited for product teams doing **pre-release security evaluations** of their LLM-powered apps. For instance, a company fine-tuning an LLM for HR chatbot use can run Promptfoo scans to ensure it doesn’t leak personal data or make disallowed promises. It’s also useful as a **continuous integration (CI) tool** – you can re-run the vulnerability suite whenever the model or prompts change, catching regressions early.

### Giskard AI – *ML Testing Platform with LLM Security Checks*

**Description:** Giskard is an open-source **evaluation and testing framework for AI models** that supports both traditional ML and LLM-based systems. It provides a Python library and UI to automatically detect issues in models, including performance bugs, bias, and security vulnerabilities. With LLMs, Giskard can scan chatbots or generative pipelines (like RAG systems) for problems like prompt injections or toxic outputs. It acts as a one-stop “QA platform” for model quality and safety.

* **Notable Features:** **One-click scanning** of an AI model via `giskard.scan()` which generates a suite of tests across different categories (robustness, fairness, security). Built-in test templates for LLMs, such as inserting **malicious instructions into knowledge base documents** (to test RAG robustness) or checking if the LLM follows guidelines. It supports any model environment (integrates with Hugging Face, OpenAI API, etc.) and can be plugged into CI pipelines. Giskard also has a GUI dashboard for reviewing issues and tracking fixes collaboratively.
* **Strengths:** **Broad scope** – it’s not limited to security, which can be convenient for teams that want a single tool to catch bias, accuracy errors, and security flaws together. Its test generation for RAG (Retrieval-Augmented Generation) is particularly useful, as it can create question-answer pairs from your knowledge base and test factuality or injection resilience automatically. Active open-source community and also backed by a startup (meaning regular updates and support).
* **Weaknesses:** Because it covers many aspects, the security tests might not be as **deep or specialized** as those in dedicated tools like Garak or Promptmap. For example, it might flag that a prompt could be injected, but not try dozens of attack variants. Using the full platform (with UI) can have a learning curve, and setting it up in CI requires some configuration.
* **Use Cases:** Giskard is useful for **ML engineers or data scientists** who want to include security testing as part of model validation without using a separate tool. After training or updating an LLM system, one can run Giskard’s scan to get an overview of any **bias issues, performance regressions, or security red flags** in one report. Particularly handy for projects where LLMs are just one component of a larger ML application, and consistency across testing tools is desired.

### LLM Guard (Protect AI) – *Input/Output Defense Toolkit*

**Description:** LLM Guard is an open-source library that acts as a **“security wrapper”** around LLM applications. Instead of actively attacking the model, LLM Guard *scans and filters* inputs and outputs in real time to prevent threats. It comes with a collection of **pluggable scanners** for common issues: prompt injection detection, harmful content filtering, sensitive data masking, etc.. Developers integrate LLM Guard into their app, and it will analyze each prompt and each response before they are processed or returned.

* **Notable Features:** **Prompt injection prevention** – detects known injection patterns and neutralizes them (for example, by stripping or sandboxing suspected instructions). **Harmful language detection** using toxicity and sentiment scanners, with options to sanitize or block such content. **Data leakage prevention** – identifies PII or secrets in prompts/responses and can redact them. **Bias detection** modules to flag biased outputs and even correct them in-line. All scanners are customizable and new ones can be added, making it extensible.
* **Strengths:** Easy to add to an existing app – it provides an SDK where you wrap your LLM API calls with LLM Guard’s scanning functions. **Comprehensive coverage of risky content**: everything from invisible Unicode text attacks to checking if the model refused when it should (a “no refusal” output scanner) is included out-of-the-box. Maintained by a dedicated AI security company (Protect AI) and fairly popular (MIT-licensed, \~1k stars).
* **Weaknesses:** LLM Guard is **defensive** in nature – it doesn’t generate new attacks, so it might not catch a vulnerability until someone attempts to exploit it. False positives can be a tuning challenge (e.g. the guard might sometimes block or alter benign outputs if they resemble a threat pattern). Also, using it may add some latency to the LLM interaction (each prompt/response gets scanned).
* **Use Cases:** Ideal for organizations that want **immediate protections for deployed LLM systems**. For example, if you’re exposing an LLM via an API to users, you can use LLM Guard to *sanitize user inputs and scrub model outputs* to prevent obvious misuse. It’s essentially a **WAF (Web Application Firewall)** but for LLMs – catching attacks in real-time. It’s also used in high-stakes domains (finance, enterprise chatbots) where any leakage of sensitive info must be prevented by design.

### Rebuff (Protect AI) – *Multi-Layer Prompt Injection Detector*

**Description:** Rebuff is another open-source tool from Protect AI, focused specifically on **prompt injection detection and mitigation**. It implements a multi-layer defense: heuristic filters, an ML-based classifier, and **canary tokens** to catch prompt leaks. Unlike LLM Guard’s broader focus, Rebuff zeroes in on detecting when an incoming prompt or context might be trying to manipulate the system, and when an outgoing response might indicate the model was tricked.

* **Notable Features:** Combines **simple rule-based filters** (e.g. regex or keyword checks for known attack phrases) with an **LLM-based detection model** that analyzes the prompt for hidden instructions. Utilizes a **vector database of known attacks**: previous injection attempts (or user-defined “bad” prompts) are embedded, so Rebuff can recognize semantically similar attacks in the future. Employs **canary tokens** – basically, planting secret phrases in system prompts so that if a prompt injection succeeds, the model’s response will contain the canary (which Rebuff catches as a leak). Integrates via simple SDK (Python or JavaScript).
* **Strengths:** Targeted specifically at one of the hardest problems (jailbreaks and injections) with a **defense-in-depth approach**. Light-weight to run – heuristics fire quickly, and the heavier LLM-check is only a second layer, so overhead is manageable. Like LLM Guard, it’s open and can be self-hosted, giving organizations control over the security logic.
* **Weaknesses:** Narrow in scope – won’t address issues like model bias or hallucination except as they relate to injection attempts. Requires maintaining the **vector database** of attacks; if an organization faces novel prompt attacks not in the DB, detection relies on the heuristic/LLM layers. As with any detection system, sophisticated prompt attackers might still bypass it by obfuscation (there’s an ongoing cat-and-mouse game).
* **Use Cases:** Rebuff is well-suited for **augmenting an LLM API’s security**. A company offering a text-generation API could install Rebuff as a middleware to check each request. It’s also useful in monitoring mode: you could run Rebuff on conversation logs to flag any incidents where a user possibly succeeded in a jailbreak (using the canary token approach to know for sure). This helps security teams quickly find and fix prompt leaks before they escalate.

### Vigil LLM – *Prompt/Response Security Analyzer*

**Description:** Vigil is an open-source Python library and REST API for detecting security issues in LLM **prompts and responses**. It’s similar in spirit to LLM Guard/Rebuff, with a modular design of scanners. Vigil’s claim to fame is using multiple techniques in parallel: it can use **YARA rules**, embedding similarity, and even small transformer models as scanners. It also supports canary tokens for leak detection. Essentially, Vigil can be invoked on any piece of text (a prompt about to go to an LLM, or a completion coming out of one) and it will return alerts if it finds indications of threats.

* **Notable Features:** **Prompt injection pattern matching** – a library of known malicious prompt patterns (like Base64-encoded instructions, attempts to reset the conversation, etc.). **Heuristic scanners** using YARA, which is a rule-based pattern matching engine often used in malware detection (here repurposed for prompt patterns). **Transformer-based classifiers** that can detect more subtle issues (e.g. a model output containing classified info it shouldn’t). Canary token insertion similar to Rebuff. Provided as both a Python library and a RESTful API, so it can plug into various infrastructures easily.
* **Strengths:** Very **flexible and extensible** – security engineers can write custom YARA rules or add new scanners to Vigil’s pipeline with ease. It’s model-agnostic and can even analyze conversation logs post-hoc. The use of multiple detection methods helps reduce false negatives (one method might catch what another misses). Being relatively lightweight, it can be run in production for real-time checks.
* **Weaknesses:** Still maturing (on the order of a few hundred stars). Using multiple scanners can increase false positives if not tuned (for instance, a very strict YARA rule might flag innocuous input). There is some overlap in functionality with more focused tools, which might lead to choosing between them (for example, if one is already using Rebuff, they may not need Vigil, or vice-versa, unless layering).
* **Use Cases:** Vigil shines in **security monitoring and forensics**. A team can feed all user prompts into Vigil’s API to log any suspicious patterns, which is helpful for detecting attack attempts in real time. It’s also used in **evaluating model outputs for safety**, e.g. scanning a batch of GPT responses to see if any contain disallowed content or leaked a hidden prompt. Because it can be self-hosted, companies use it to analyze data **without sending it to external services**, preserving privacy while checking security.

## LLM Red-Teaming Frameworks

Beyond automated scanners, there are more extensive **red-teaming frameworks** that facilitate manual or semi-automated adversarial testing of LLMs. These frameworks often incorporate human expertise and complex attack strategies, enabling testers to craft multi-turn conversations or novel exploits. They tend to support **broader scenario-based evaluations** – not just one-off prompts, but entire dialogs or agent behaviors. Many also allow integration of custom attack tactics and integrate with evaluation metrics. Below are key open-source frameworks:

### DeepTeam (Confident AI) – *Open-Source Adversarial Testing Framework*

**Description:** DeepTeam is a **purpose-built LLM red-teaming framework** that incorporates the latest research on attacking LLMs. It’s designed to simulate how malicious users might interact with your system in both single-turn and multi-turn conversations. Out-of-the-box, DeepTeam provides **40+ vulnerability types** to test for – spanning **bias (gender, race, political, etc.), PII leakage (direct and via session or databases), misinformation (factual errors, unsupported claims), robustness issues (prompt hijacking, input manipulation), and more**. To trigger these vulnerabilities, it includes **10+ attack methods** (techniques) such as classic prompt injections, various **jailbreaking styles** (linear, tree, crescendo), leetspeak obfuscation, role-play exploits, etc..

* **Notable Features:** DeepTeam leverages LLMs themselves to generate attacks – it can use a large model to help craft an adversarial input tailored to break a specific vulnerability. It supports **multi-turn attack simulations**, meaning it can carry on a dialogue with the target model, escalating the attack in steps (this is important for testing conversational jailbreaks that require context and trickery over multiple messages). DeepTeam integrates with the **DeepEval** framework for evaluation, and it can output structured **risk assessment reports** (e.g. as dataframes or JSON) summarizing which vulnerabilities were successfully induced. It also aligns with standard guidelines (like the OWASP Top-10 for LLMs), so testers can map findings to those categories.
* **Strengths:** Very **comprehensive and customizable** – you can easily add your own custom vulnerability definitions or attack strategies in code (just a few lines, as per documentation). Because it runs locally and you plug in your own model callback, it works with **any model – open-source or closed** – as long as you can query it via Python. The framework handles orchestration, so testers can focus on designing new attacks or interpreting results rather than building tooling. Confident AI (the maintainers) actively update it with new research findings, so it stays cutting-edge.
* **Weaknesses:** Using DeepTeam requires some **coding and ML background** – you need to write a small Python snippet to wrap your model as a `model_callback` and possibly to interpret results. It’s not a one-click scanner; rather, it’s a testing harness (more flexible but a bit less plug-and-play than, say, Garak). Also, because it may use large LLMs to generate attack prompts, it could require API access to powerful models or running a local attacker model, which can be resource-intensive.
* **Use Cases:** Ideal for AI safety researchers and internal red teams who want to **thoroughly probe an LLM system’s defenses**. For example, before deploying a chatbot to millions of users, a company’s red team can use DeepTeam to perform realistic attack simulations – e.g., see if a user can slowly social-engineer the bot into revealing another user’s data over a conversation. It’s also used in benchmarks and competitions (like red-teaming events) to automate attack discovery, and in academic research to test new classes of attacks in a systematic way.

### PyRIT (Microsoft) – *Python Risk Identification Toolkit*

**Description:** PyRIT (Python Risk Identification Tool) is an open-source framework from Microsoft’s Azure AI Red Team, built to **proactively identify risks in generative AI systems**. It provides a structured way to carry out attacks and evaluate outcomes, with a particular emphasis on automating what Microsoft calls *AI Red Teaming* tasks. PyRIT is quite extensive – it includes orchestrators for different attack strategies and can scale up testing (there are cookbooks for sending millions of prompts, for example).

* **Notable Features:** **Diverse attack orchestrators** – e.g. a *Cross-Domain Prompt Injection Attack (XPIA)* orchestrator that tests injecting hidden instructions via different input channels, a *Many-Shot Jailbreaking* orchestrator to try complex multi-prompt jailbreaks, a *Tree-of-Thoughts* style attack planner, and even some whimsical ones like “Violent Durian” (an attack strategy demo). PyRIT supports **multi-turn dialogues, role-play scenarios, and tool usage testing** (it can simulate an LLM agent with tools and see if it can be misled into unsafe actions). It’s highly extensible: you can script new orchestrators or targets fairly easily. It has configurations for targeting OpenAI chat models, HuggingFace local models, and even web targets or non-LLM components. PyRIT also integrates logging and scoring mechanisms to quantify risk.
* **Strengths:** PyRIT comes straight from a seasoned internal red team, so it **embodies real-world red-teaming knowledge**. It has a large community (2.5k+ stars) and is actively updated by Microsoft, including detailed docs and examples. It’s arguably one of the **most powerful and flexible** LLM attack frameworks – you can orchestrate very sophisticated test sequences that mimic advanced attackers. Good integration with Azure services (for those in Microsoft’s ecosystem) but not limited to them.
* **Weaknesses:** With great power comes complexity – PyRIT has a **steeper learning curve** than simpler scanners. Setting up orchestrations and analyzing their results might require more effort, and it’s probably overkill for small-scale testing. Some of its advanced features assume usage of Azure OpenAI or certain infrastructure (though they can be adapted). It’s also primarily code-driven; there’s no simple UI, which may not appeal to all users.
* **Use Cases:** PyRIT is well-suited for **comprehensive security evaluations by experienced teams**. Think of a scenario like testing a new AI feature in Microsoft Office: the red team could use PyRIT to throw the kitchen sink of attacks at it – from prompt manipulation in documents (cross-domain injection) to chaining model responses in loops to see if it breaks policy. It’s also a great learning tool – security researchers use PyRIT to **reproduce known exploits and verify fixes** across different models, and to contribute new attack techniques to the community.

### OpenAI Evals – *Evaluation Harness (Including Red-Teaming Tasks)*

**Description:** OpenAI Evals is an open-source framework for evaluating LLMs and LLM systems, with a registry of community-contributed evals. While not solely a security tool, it can be (and has been) used to create **custom red-teaming evaluations**. For instance, one can write an eval that tests whether a model follows OpenAI policy by attempting various disallowed requests. Many of OpenAI’s own internal stress tests (for GPT-4 etc.) were implemented as Evals. The framework allows for both automated prompt evaluations and **human-in-the-loop** judgments.

* **Notable Features:** **Highly customizable eval scripts** – you can define an evaluation in Python that sends a series of prompts and checks the model’s outputs against expected criteria (using regex, exact match, or even another AI model as a judge). It supports **multi-turn dialogues** and can track model state between turns. A growing **registry of benchmarks** means you might find existing security-relevant evals (for instance, there are evals for whether a model reveals a secret string when prompted, etc.). It is designed to handle batch evaluations and output aggregate metrics (like “model failed X out of Y attempts”).
* **Strengths:** Officially maintained by OpenAI and widely used by the community – there’s a lot of knowledge and examples to draw on. It’s model-agnostic (though originally made for OpenAI’s API, it can be extended to local models by writing a custom model adapter). Great for **standardizing evaluations**: if you create a security eval, you can run it across many models to directly compare their behavior. Also, contributions are open, so one can share new red-team evals with others.
* **Weaknesses:** It’s more of a **framework than a ready tool** – you have to write code for your specific eval logic. There’s no predefined vulnerability suite here (unless you import someone else’s from the registry). The learning curve exists in understanding the evals format and writing prompts programmatically. For someone just wanting a quick scan, Evals would be too involved.
* **Use Cases:** Useful for **benchmarking model safety** – e.g. an academic might create an eval that measures how often different models refuse to output hate speech when prompted, and run it via Evals to compare models. OpenAI Evals was used during the GPT-4 release to let users contribute red-team tests; a company could similarly use it internally to crowdsource red-team scenarios and run them automatically. It’s also employed to ensure **continuity of safeguards** – for example, after fine-tuning a model, re-run a suite of Evals (covering prompts about self-harm, profanity, etc.) to confirm it hasn’t regressed in safety compliance.

### Other Noteworthy Tools and Frameworks

* **NeMo Guardrails (NVIDIA):** An open-source toolkit to add **programmable guardrails** to LLM-based apps. Developers can specify rules for allowable model behavior (for instance, what the assistant should never say or do), and NeMo Guardrails enforces these at runtime by modulating prompts or responses. While not a vulnerability scanner, it helps *prevent misuse and certain attacks* by constraining the model. Strength: integrates with popular frameworks (LangChain, etc.) and offers conversational AI policy templates. Weakness: requires careful rule design and can be bypassed if misconfigured. Use case: ensuring an enterprise chatbot stays on topic and refuses out-of-scope requests (a form of **preventative security**).
* **Guardrails AI (Shreya Rajpal):** A Python library for validating and constraining LLM outputs. It uses a declarative YAML schema to define what output is expected (format, content constraints) and automatically **catches deviations** – for example, if the model’s answer contains disallowed content or is missing a required field. As a security tool, it can detect and stop some prompt injection effects (e.g. if the model starts replying in a way that breaks the expected schema) and flag toxic content. It comes with many pre-built validators (for toxicity, bias, etc.). Typically used to **shield applications from errant model behavior** by verifying outputs before they reach the user.
* **Invariant (AI Trace Scanner):** An emerging tool focused on **monitoring agent execution traces for security bugs**. When you have an AI agent that takes actions (like code execution, web browsing), Invariant scans the sequence of steps to detect things like data leaks, insecure API calls, or injected commands. It’s more specialized, useful in detecting issues **after deployment** by analyzing logs. This is a newer approach to catch sophisticated multi-step attacks that may not be evident from a single prompt/response.

## Comparison of Tools

The table below summarizes key differences among several open-source LLM security tools, comparing their focus, supported models, ease of use, extensibility, and maintenance status:

| **Tool**              | **Primary Focus & Coverage**                                                                                                                                                                                                                                                                  | **Model Support**                                                                                                                            | **Ease of Use & Extensibility**                                                                                                                                                                                                                                                                                                           | **Active Maintenance**                                                                                                                                                 |
| --------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Garak**             | Broad vulnerability scanner (prompt injection, data leak, hallucination, toxicity, jailbreak, etc.). CLI tool probes models with diverse attack plugins (incl. encoding attacks, XSS).                                                                                                        | HuggingFace models, OpenAI/Anthropic APIs, Replicate, local GGUF (LLaMA.cpp), etc..                                                          | **Ease:** Pip-installable, run via single CLI command; good docs and examples. **Extensibility:** Highly extensible plugin architecture (users can add custom probe modules; supports new model interfaces easily).                                                                                                                       | Very active (NVIDIA-backed, 1000+ stars, frequent updates). Discord community and ongoing research integration (DefCon, arXiv paper).                                  |
| **Promptmap**         | Specialized scanner for **prompt injection** vulnerabilities (system prompt leaking, role overrides). Uses preset attack rules (prompt stealing, distraction, etc.) to test if model can be manipulated.                                                                                      | OpenAI (GPT-3.5/4), Anthropic (Claude), Open-source via Ollama (LLaMA 2, Mistral, etc.).                                                     | **Ease:** Requires providing system prompts; run via Python script with flags (no coding needed). Clear output of pass/fail for each attack. **Extensibility:** Custom rules definable in YAML; moderate flexibility within injection domain.                                                                                             | Active (rewritten in 2025, maintained by author). Medium community engagement (niche use-case).                                                                        |
| **LLMFuzzer**         | **Fuzz testing** framework – generates high-volume random and mutated inputs to stress-test LLMs and integrations. Uncovers edge-case failures and robustness issues (incl. crashes or slowdowns, besides injections).                                                                        | API-based models (OpenAI, etc.), custom integration via HTTP. Can target any LLM accessible through an API or SDK.                           | **Ease:** Technical to set up fuzzing campaigns; requires defining target and possibly analysis of logs. **Extensibility:** New fuzz strategies and input generators can be added (modular design). Connectors for various API interfaces (GET/POST, etc.) planned.                                                                       | Moderate activity (early-stage project, \~200 stars). Maintained by independent developers with sporadic updates.                                                      |
| **Promptfoo Scanner** | **Adversarial prompt testing** with policy compliance focus. Covers PII leaks, jailbreaks, unauthorized actions, hallucinations, brand risks, etc.. Auto-generates test cases tailored to your app using AI.                                                                                  | Model-agnostic – can test any model via API or SDK. Often used with OpenAI, Anthropic, or custom LLMs by wrapping their calls.               | **Ease:** Very user-friendly (CLI & web UI, detailed reports). Minimal setup – point at your prompt/config and run. **Extensibility:** Test scenarios auto-generated; also supports custom prompt templates. Open-source core with enterprise addons.                                                                                     | Active (community + company support, regular releases). Growing adoption in industry; likely to stay maintained long-term.                                             |
| **Giskard**           | **General AI testing platform** with security checks. Scans for bias, performance issues, and LLM-specific flaws like prompt injection or toxic output. Not as in-depth in any single category, but broad overall QA for models.                                                              | Many environments: local models (via Python), remote APIs, and even non-LLM models. Integrates with HuggingFace, OpenAI, LangChain, etc.     | **Ease:** Offers both Python API and a GUI dashboard. One-command scanning available, but full platform setup for GUI is heavier. **Extensibility:** Custom tests can be written in Python; plugin system for new metric or test types.                                                                                                   | Active (startup-backed open source, \~1k stars). Frequent updates, especially for compatibility with new LLM releases.                                                 |
| **LLM Guard**         | **Defensive middleware** – scans and sanitizes inputs/outputs at runtime. Focus on prompt injection prevention, toxic content filtering, PII masking, etc. Includes many built-in scanners (regex, forbidden topics, length checks, etc.).                                                    | Model-agnostic (works as a wrapper around any LLM API or call). Often used with OpenAI, Anthropic, or open models in apps.                   | **Ease:** Integrated via a few lines of code around your LLM calls. Pre-built scanners work out-of-box; configuration via YAML or code to enable/disable checks. **Extensibility:** New scanner modules can be added (follows a plugin pattern). Composable input/output filtering policies.                                              | Active (Protect AI backed, \~1k stars). Receives updates for new attack patterns and scanner improvements. Likely sustained by enterprise interest.                    |
| **Rebuff**            | **Prompt injection detection** with multi-layer approach (heuristics + an ML model + canary tokens). Specifically watches for attempts to bypass instructions or reveal secrets.                                                                                                              | Model-agnostic (deployed as an input/output filter in any LLM app).                                                                          | **Ease:** Provided as lightweight SDK (Python/JS). Minimal config needed to start detecting known injection patterns. **Extensibility:** Attack signature DB can be updated; supports custom heuristics or embedding new known exploits for recognition.                                                                                  | Active (Protect AI, Apache-2.0). Less broad usage than LLM Guard, but maintained in tandem. Frequent updates aligning with emerging prompt attack techniques.          |
| **Vigil**             | **Security analysis library** for prompts/responses. Uses heuristics (YARA rules) and ML to flag prompt injections, jailbreak attempts, content policy violations, etc.. Also employs canary tokens for leak detection.                                                                       | Model-agnostic. Can be used offline on logs or inline in applications.                                                                       | **Ease:** Use via Python or REST API. Requires choosing which scanners to enable. **Extensibility:** Highly extensible – users can write custom rules or integrate new detection models. Designed for flexibility in scanning different threat types.                                                                                     | Moderate activity (open-source project with \~200 stars). Maintained by security researchers (deadbits); updates as new threats are researched.                        |
| **DeepTeam**          | **Adversarial red-teaming framework** – tests 40+ vulnerability types (bias, privacy, factuality errors, robustness issues, etc.) with 10+ attack techniques (prompt injections, multi-turn jailbreaks, roleplay attacks, etc.). Generates sophisticated multi-turn attack scenarios.         | Any LLM system (user provides a Python callback to call their model – supports open models, OpenAI, Anthropic, agents, etc.).                | **Ease:** Requires Python use; good documentation and examples for setup. Automates attack generation and evaluation once configured. **Extensibility:** Very extensible – new vulnerabilities or attack methods can be added with simple Python classes. Built to be customized for different domains.                                   | Active (Confident AI, 2024 release). Still growing community, but frequent commits and rapid incorporation of latest research.                                         |
| **PyRIT**             | **Full-spectrum AI Red Team toolkit** from Microsoft. Covers numerous harm categories and complex attack strategies (cross-domain injections, multi-turn social engineering, tool misuse, etc.). Orchestrates large-scale evaluations (up to millions of prompts) with advanced attack trees. | Many targets: OpenAI chat models, Azure OpenAI, HuggingFace models, web endpoints, multi-modal agents. Highly configurable target interface. | **Ease:** High learning curve – essentially a framework where you script scenarios. Strong documentation and examples help, but not a plug-and-play scanner. **Extensibility:** Extremely extensible – users can create custom orchestrators, strategies, and integrate new model targets. Meant for experienced red teamers to build on. | Very active (Microsoft-backed, 2.5k+ stars). Continuous development with community contributions. Widely used in AI security research and likely to remain a standard. |
| **OpenAI Evals**      | **Evaluation framework** (not specific to security, but supports security evals). Community-sourced tests for model behavior, including some red-team style prompts. Useful for regression testing and cross-model comparisons.                                                               | OpenAI API by default; extensible to others via custom model adapters.                                                                       | **Ease:** Requires writing Python eval scripts; not an out-of-box scanner. However, many example evals exist to copy/modify. **Extensibility:** Unlimited – any scenario or metric can be coded. Integrates human evaluation for nuanced cases.                                                                                           | Active (OpenAI-maintained, with community contributions). Used in OpenAI’s own releases; ongoing improvements and an evolving eval repository.                         |

**Table Legend:** *“Model Support”* indicates which LLM platforms the tool can work with (closed-source APIs, open models, etc.). *“Ease of Use”* covers installation, interface (CLI, library, GUI), and how much configuration/coding is needed. *“Extensibility”* notes how easily one can add new tests or adapt the tool to new threat types. *“Active Maintenance”* reflects community interest and recency of updates (as of 2024/2025).

## Conclusion

In summary, the ecosystem of LLM security tools is rapidly growing and **each tool has its niche**:

* **Automated scanners** like Garak and Promptfoo offer quick broad sweeps of model vulnerabilities, great for initial assessments and continuous integration checks. They are relatively easy to run but may trade depth in specific areas.
* **Specialized tools** such as Promptmap or Rebuff hone in on prompt injections – crucial since prompt injection (OWASP LLM01) remains one of the top threats. These are indispensable for any application with system prompts or chain-of-thought that must remain hidden.
* **Defensive frameworks** (LLM Guard, Guardrails, etc.) provide runtime protection, acting as a safety net. While they don’t *find* vulnerabilities, they help **mitigate** them in production by filtering and constraining model behavior.
* **Red-teaming frameworks** like DeepTeam and PyRIT are more involved but enable a **thorough and creative exploration** of an LLM’s failure modes. They shine in the hands of experienced testers who can configure nuanced attack scenarios and interpret complex outcomes.
* **General AI testing platforms** (Giskard, OpenAI Evals) integrate security testing into a broader QA or benchmarking workflow, ensuring that security is checked alongside accuracy, fairness, and other quality metrics.

When choosing a tool, consider your **use case and resources**. For a quick sanity check on a new chatbot, an automated scanner (with a focus on your key risk, be it data leakage or toxicity) is ideal. For ongoing protection of a live system, implementing guardrails or detectors is wise. And for high-security applications or research into model robustness, a full red-team exercise with frameworks like DeepTeam or PyRIT will provide the deepest insights.

Finally, it’s worth noting that these tools are complementary. Many organizations employ **multiple layers**: e.g. using Garak or Promptfoo in development to catch obvious issues, deploying LLM Guard in production for real-time defense, and occasionally running a PyRIT/DeepTeam analysis for a deep dive. Given the pace of AI advancements and threat evolution, staying updated with tool improvements and community findings is crucial – *secure AI development is an ongoing process*, and the open-source community is playing a pivotal role in it. By leveraging the strengths of these tools, developers and security teams can significantly enhance their LLM’s resilience against emerging attacks.

**Sources:** The information above was synthesized from official documentation and articles for each tool, including GitHub README pages and technical blogs, as well as the OWASP Top 10 guidelines for LLM applications. Each tool’s name links to its source repository or website for further details.

###


## Summary Extracts from [Insights and Current Gaps in Open-Source LLM Vulnerability Scanners: A Comparative Analysis](https://arxiv.org/abs/2410.16527)

  * **[Garak](https://arxiv.org/html/2406.11036v1)**

    > Garak offers the most extensive test-suite, making it suitable for red-teaming groups that deal with diverse use-cases. However, it focuses more on a static attack dataset, limiting customizability. Garak also integrates with Nvidia’s NeMo Guardrails, enabling setting additional safety layers.

  * **[Giskard](https://docs.giskard.ai/en/stable/open_source/scan/scan_llm/index.html)**

    > Giskard is ideal for users seeking flexible attack generation with both static and LLM-based methods. It offers a simple yet effective customization of LLM-based attacks via userprovided natural language context - enabling tailored test suites for various attack types, useful for dynamic online

  * **[PyRIT](https://www.google.com/search?q=https://arxiv.org/pdf/2410.02828)**

    > PyRIT offers the most customizable test suite, focusing on LLM-based attacks. It allows users to edit both attacker and evaluator LLMs, providing full access to their instructions. This offers extensive flexibility but requires significant prompt engineering. Therefore, PyRIT is best suited for red-teams focusing on an internally crafted test-suite rather than relying on external knowledge. PyRIT’s distinctive multi-step attacks and rich ’semi-automatic’ options provide an additional edge.

  * **[CyberSecEval](https://arxiv.org/pdf/2404.13161)**

    > CyberSecEval focuses on red-teaming for code-generating LLMs and is placed in the ready-to-use end of the spectrum. Its test-suite is designed to expose code-related security issues. This makes it valuable for red-teaming groups dealing with generative AI for software and cybersecurity, where the integrity of auto-generated code is crucial.



  ## Notes


## Summary Extracts from [Insights and Current Gaps in Open-Source LLM Vulnerability Scanners: A Comparative Analysis](https://arxiv.org/abs/2410.16527)

  * **[Garak](https://arxiv.org/html/2406.11036v1)**

    > Garak offers the most extensive test-suite, making it suitable for red-teaming groups that deal with diverse use-cases. However, it focuses more on a static attack dataset, limiting customizability. Garak also integrates with Nvidia’s NeMo Guardrails, enabling setting additional safety layers.

  * **[Giskard](https://docs.giskard.ai/en/stable/open_source/scan/scan_llm/index.html)**

    > Giskard is ideal for users seeking flexible attack generation with both static and LLM-based methods. It offers a simple yet effective customization of LLM-based attacks via userprovided natural language context - enabling tailored test suites for various attack types, useful for dynamic online

  * **[PyRIT](https://www.google.com/search?q=https://arxiv.org/pdf/2410.02828)**

    > PyRIT offers the most customizable test suite, focusing on LLM-based attacks. It allows users to edit both attacker and evaluator LLMs, providing full access to their instructions. This offers extensive flexibility but requires significant prompt engineering. Therefore, PyRIT is best suited for red-teams focusing on an internally crafted test-suite rather than relying on external knowledge. PyRIT’s distinctive multi-step attacks and rich ’semi-automatic’ options provide an additional edge.

  * **[CyberSecEval](https://arxiv.org/pdf/2404.13161)**

    > CyberSecEval focuses on red-teaming for code-generating LLMs and is placed in the ready-to-use end of the spectrum. Its test-suite is designed to expose code-related security issues. This makes it valuable for red-teaming groups dealing with generative AI for software and cybersecurity, where the integrity of auto-generated code is crucial.