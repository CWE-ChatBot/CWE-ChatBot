# Epic 3.1 Implementation Plan: Advanced User Input & Context Preservation

## Overview

Story 3.1 focuses on implementing advanced user input handling and context preservation for the CWE ChatBot. This story builds upon the existing comprehensive implementation in the apps/chatbot directory to add enhanced input processing, file upload capabilities, conversational context preservation, and role-based response tailoring. Based on analysis of the existing codebase, significant functionality is already implemented, but specific gaps need to be filled to meet all acceptance criteria.

## Existing Implementation Analysis

### What's Already Implemented

**Core Infrastructure (✅ COMPLETE):**
- **main.py**: Full Chainlit application with persona selection, file upload, settings UI, role management
- **input_security.py**: InputSanitizer and SecurityValidator with comprehensive prompt injection protection
- **file_processor.py**: Complete file upload handling supporting PDF/text with virus scanning and size limits
- **user_context.py**: Complete UserContext and UserPersona management with 7 personas
- **conversation.py**: Session management with cl.user_session integration and conversation flow
- **query_handler.py**: Production integration with Story 1.5 hybrid retrieval system
- **response_generator.py**: Role-based response generation with persona-specific templates

**Acceptance Criteria Coverage:**

**AC1 (Input patterns) - ✅ IMPLEMENTED:**
- InputSanitizer handles multiple input formats in `sanitize_input()` method
- Detects vulnerability descriptions, CVE advisories, tool outputs
- Evidence: Lines 80-169 in input_security.py

**AC2 (File upload) - ✅ IMPLEMENTED:**
- FileProcessor provides secure file upload with validation
- Supports PDF, text, markdown, JSON up to 10MB
- Virus scanning and malicious content detection
- Evidence: Lines 41-89 in file_processor.py, Lines 469-503 in main.py

**AC3 (Data privacy) - ✅ IMPLEMENTED:**
- File content isolation using cl.user_session storage
- Content never merged into prompts, stored as separate evidence
- Evidence: Lines 267-286 in main.py

**AC4 (Context preservation) - ✅ IMPLEMENTED:**
- Session context in cl.user_session with UserContext management
- Conversation history preservation across sessions
- Evidence: Lines 80-101 in conversation.py, Lines 60-91 in user_context.py

**AC5 (Size limits) - ✅ IMPLEMENTED:**
- File size limits (10MB) and content validation
- Input length validation (2000 chars) with graceful handling
- Evidence: Lines 35-40 in file_processor.py, Lines 121-126 in input_security.py

**AC11 (Role selection) - ✅ IMPLEMENTED:**
- Persona selection UI in top bar and settings panel
- 7 personas: PSIRT Member, Developer, Academic Researcher, Bug Bounty Hunter, Product Manager, CWE Analyzer, CVE Creator
- Evidence: Lines 120-126 in main.py, Lines 17-37 in user_context.py

**AC12 (Role-based responses) - ✅ IMPLEMENTED:**
- Persona-specific preferences and response focus
- Different section boosts and content prioritization per role
- Evidence: Lines 92-161 in user_context.py

**AC13 (Hallucination mitigation) - ✅ PARTIALLY IMPLEMENTED:**
- InputSanitizer prevents prompt injection attacks
- Response validation in SecurityValidator
- Evidence: Lines 317-409 in input_security.py

### Implementation Gaps Identified

**AC6 (Comprehensive CWE retrieval) - 🔶 PARTIALLY IMPLEMENTED:**
- Basic retrieval exists but needs enhancement for comprehensive metadata
- Missing structured presentation of relationships and consequences
- Current: Basic chunk retrieval in query_handler.py
- Gap: Comprehensive CWE metadata extraction and formatting

**AC7 (Follow-up questions) - 🔶 PARTIALLY IMPLEMENTED:**
- Conversation context tracking exists but follow-up intent detection missing
- Current: Conversation history in UserContext
- Gap: Follow-up intent detection and context-aware processing

**AC8 (Factual accuracy) - 🔶 PARTIALLY IMPLEMENTED:**
- CWE metadata extraction exists but needs verification against source
- Current: Basic metadata in retrieved chunks
- Gap: Fact verification and source attribution system

**AC9 (Similar CWEs) - ❌ NOT IMPLEMENTED:**
- No similar CWE discovery logic implemented
- Gap: Relationship-based CWE discovery using corpus relationships

**AC10 (Progressive disclosure) - ❌ NOT IMPLEMENTED:**
- No summary/detail response formatting implemented
- Gap: Chainlit action buttons for "show more details" functionality

**AC14 (Confidence scoring) - ❌ NOT IMPLEMENTED:**
- No confidence score calculation or display
- Gap: Vector similarity to confidence conversion and UI display

**AC15 (Low-confidence handling) - ❌ NOT IMPLEMENTED:**
- No low-confidence detection or query refinement suggestions
- Gap: Confidence threshold handling and user guidance

## Pre-Implementation Checklist

### Prerequisites Verification
- [x] **Story 1.5 Infrastructure**: Production CWE ingestion pipeline with 7,913 chunks operational
- [x] **Chainlit Framework**: Version 0.7.x with native UI elements and file upload support
- [x] **Database Access**: PostgreSQL with pgvector and halfvec optimization functional
- [x] **API Access**: Gemini API key configured for embeddings and response generation
- [x] **Security Framework**: Input sanitization and validation infrastructure in place

### Tool/Environment Requirements
- [x] **Poetry**: Dependency management with virtual environment
- [x] **pytest**: Test framework with existing comprehensive test suite
- [x] **Playwright**: E2E testing framework with existing Chainlit UI tests
- [x] **Development Environment**: Python 3.10+ with all dependencies installed

### Access Requirements
- [x] **Production Database**: Google Cloud SQL access with IAM authentication
- [x] **Gemini API**: API key with sufficient quota for production usage
- [x] **Codebase Access**: Full read/write access to apps/chatbot directory

### Architecture/Design Dependencies
- [x] **Story 1.5 Components**: PostgresChunkStore and GeminiEmbedder integration
- [x] **UserContext System**: Persona management and session state infrastructure
- [x] **Security Validation**: Input sanitization and response validation pipelines

### Security Considerations
- [x] **Input Validation**: Comprehensive prompt injection and command injection protection
- [x] **File Upload Security**: Virus scanning, type validation, and size limits
- [x] **Session Isolation**: Proper context isolation between users
- [x] **Data Privacy**: Secure handling of user-submitted confidential information

## Implementation Steps

### Phase 0: Foundation & Ship Blockers (Estimated: 4 hours)

#### Step 0.1: Feature Flags and Configuration System
**CRITICAL**: Address ship blockers identified in plan review

```python
# In config.py - Add feature flags and data egress controls
@dataclass
class FeatureFlags:
    """Feature flags for Story 3.1 functionality."""
    confidence_display_enabled: bool = True
    followup_enabled: bool = True
    similar_cwe_mode: str = "relationships"  # or "regex"
    data_egress_mode: str = "embeddings_only"  # "none", "embeddings_only", "full"

# Environment variable mapping
def load_feature_flags() -> FeatureFlags:
    return FeatureFlags(
        confidence_display_enabled=os.getenv("CONFIDENCE_DISPLAY_ENABLED", "true").lower() == "true",
        followup_enabled=os.getenv("FOLLOWUP_ENABLED", "true").lower() == "true",
        similar_cwe_mode=os.getenv("SIMILAR_CWE_MODE", "relationships"),
        data_egress_mode=os.getenv("DATA_EGRESS_MODE", "embeddings_only")
    )

# Data egress enforcement
class DataEgressValidator:
    def __init__(self, mode: str):
        self.mode = mode

    def validate_embedding_request(self, content: str) -> bool:
        """Validate if embedding request is allowed."""
        if self.mode == "none":
            return False
        return True  # embeddings_only and full allow embeddings

    def validate_llm_request(self, content: str) -> bool:
        """Validate if LLM request is allowed."""
        if self.mode in ["none", "embeddings_only"]:
            return False
        return True  # only full allows LLM requests
```

#### Step 0.2: Type Contracts and Models
**Implementation**: Add strongly-typed interfaces

```python
# In models.py - Add type-safe contracts
from typing import TypedDict, Literal
from dataclasses import dataclass

@dataclass
class Chunk:
    """Strongly-typed chunk model."""
    document: str
    metadata: dict  # {cwe_id: str, name: str, section: str}
    scores: dict    # {hybrid: float}

class QueryResult(TypedDict):
    """Type-safe query result contract."""
    chunks: list[Chunk]
    confidence_score: float
    confidence_level: Literal["Very Low", "Low", "Medium", "High"]
    similar_cwes: list[dict]  # optional

class ConfidenceFactors(TypedDict):
    """Contributing factors for confidence calculation."""
    avg_similarity: float
    result_count: float
    query_specificity: float
    source_diversity: float
```

#### Step 0.3: Evidence Isolation & Cleanup Guards
**CRITICAL**: Ensure evidence never leaks into prompts

```python
# In conversation.py - Enhanced evidence cleanup
def _ensure_evidence_cleanup(self):
    """Ensure evidence is cleared from session after generation."""
    try:
        cl.user_session.set("uploaded_file_context", None)
        cl.user_session.set("last_detailed_response", None)
        logger.debug("Evidence context cleared from session")
    except Exception as e:
        logger.error(f"Failed to clear evidence context: {e}")

async def process_user_message_streaming(self, session_id: str, message_content: str, message_id: str):
    try:
        # ... existing logic ...

        # Generate response
        response = await self.response_generator.generate_response_streaming(...)

        return result
    finally:
        # CRITICAL: Always clean up evidence
        self._ensure_evidence_cleanup()

# Add validation that evidence never enters prompts directly
def _validate_no_evidence_in_prompt(self, prompt: str) -> bool:
    """Validate that evidence content doesn't appear in LLM prompts."""
    evidence_markers = [
        "<<FILE_CONTEXT_START>>",
        "uploaded_file_context",
        "Attached File Content"
    ]
    return not any(marker in prompt for marker in evidence_markers)
```

#### Step 0.4: Control Character and Security Hardening
**Implementation**: Strip dangerous content from evidence

```python
# In input_security.py - Enhanced evidence sanitization
def sanitize_evidence_content(self, content: str) -> str:
    """Sanitize user-uploaded evidence content."""

    # Strip control characters except common whitespace
    content = re.sub(r'[\x00-\x08\x0B-\x0C\x0E-\x1F\x7F]', '', content)

    # Neutralize markdown autolinks
    content = re.sub(r'\[([^\]]+)\]\(javascript:[^\)]+\)', r'\1', content)
    content = re.sub(r'\[([^\]]+)\]\(data:[^\)]+\)', r'\1', content)

    # Strip obvious secrets from evidence
    secrets_patterns = [
        r'AKIA[0-9A-Z]{16}',           # AWS Access Key
        r'xoxp-[0-9]+-[0-9]+-[0-9]+-[a-f0-9]+',  # Slack token
        r'-----BEGIN PRIVATE KEY-----',  # Private keys
        r'sk-[a-zA-Z0-9]{48}'          # OpenAI-style keys
    ]

    for pattern in secrets_patterns:
        content = re.sub(pattern, '[REDACTED_SECRET]', content, flags=re.IGNORECASE)

    return content
```

### Phase 1: Enhanced CWE Retrieval System (Estimated: 3-4 hours)

#### Step 1.1: Enhance Comprehensive CWE Metadata Retrieval (AC6)
**Current State**: Basic chunk retrieval exists in query_handler.py
**Enhancement Needed**: Extract and format comprehensive CWE metadata

```python
# In query_handler.py - Enhance process_query method
async def process_query_comprehensive(self, query: str, user_context: Dict[str, Any]) -> List[Dict[str, Any]]:
    """Enhanced query processing with comprehensive metadata extraction."""

    # Get basic retrieval results
    chunks = await self.process_query(query, user_context)

    # Group chunks by CWE and extract comprehensive metadata
    cwe_metadata = {}
    for chunk in chunks:
        cwe_id = chunk["metadata"]["cwe_id"]
        if cwe_id not in cwe_metadata:
            cwe_metadata[cwe_id] = {
                "description": [],
                "consequences": [],
                "relationships": [],
                "detection_methods": [],
                "mitigation": []
            }

        # Extract section-specific content
        section = chunk["metadata"].get("section", "")
        content = chunk["document"]

        if "description" in section.lower():
            cwe_metadata[cwe_id]["description"].append(content)
        elif "consequence" in section.lower():
            cwe_metadata[cwe_id]["consequences"].append(content)
        # ... continue for other sections

    # Enhance chunks with comprehensive metadata
    for chunk in chunks:
        chunk["comprehensive_metadata"] = cwe_metadata.get(
            chunk["metadata"]["cwe_id"], {}
        )

    return chunks
```

#### Step 1.2: Implement Related CWE Discovery (AC9)
**UPDATED**: Use relationship data instead of regex when possible

```python
# New method in CWEQueryHandler with improved relationship handling
async def find_similar_cwes(self, cwe_id: str, limit: int = 5) -> List[Dict[str, Any]]:
    """Find CWEs related to the given CWE using structured relationship metadata."""

    # Check feature flag for implementation mode
    feature_flags = load_feature_flags()

    if feature_flags.similar_cwe_mode == "relationships":
        return await self._find_similar_via_relationships(cwe_id, limit)
    else:
        return await self._find_similar_via_regex(cwe_id, limit)

async def _find_similar_via_relationships(self, cwe_id: str, limit: int = 5) -> List[Dict[str, Any]]:
    """Preferred: Use structured relationship sections only."""

    # Query specifically for relationship/taxonomy sections
    relationship_query = f"CWE-{cwe_id}"
    relationship_chunks = await self.store.hybrid_search(
        query_embedding=await self.embedder.get_embedding(relationship_query),
        query_text=relationship_query,
        weights=self.hybrid_weights,
        limit=20,
        # Filter to only relationship sections if metadata supports it
        metadata_filter={"section": ["Relationships", "Taxonomy Mappings"]}
    )

    # Extract related CWE IDs from relationship content with scope limiting
    related_cwes = set()
    for chunk in relationship_chunks:
        # Only process if section is actually about relationships
        section = chunk.get("metadata", {}).get("section", "").lower()
        if "relationship" in section or "taxonomy" in section:
            content = chunk["document"]
            # Extract CWE-### patterns, being more precise
            import re
            cwe_matches = re.findall(r'\bCWE-(\d+)\b', content)
            related_cwes.update(cwe_matches)

    # Remove the original CWE and limit results
    related_cwes.discard(cwe_id)

    # Cache results to avoid repeated computation
    cache_key = f"similar_cwes:{cwe_id}"
    similar_results = []

    for related_id in list(related_cwes)[:limit]:
        # Get cached or retrieve basic info for related CWEs
        related_chunks = await self.store.hybrid_search(
            query_embedding=await self.embedder.get_embedding(f"CWE-{related_id}"),
            query_text=f"CWE-{related_id}",
            weights=self.hybrid_weights,
            limit=2  # Just enough for name and basic info
        )
        if related_chunks:
            similar_results.append({
                "cwe_id": f"CWE-{related_id}",
                "name": related_chunks[0].get("metadata", {}).get("name", f"CWE-{related_id}"),
                "relationship_type": "related",
                "confidence": related_chunks[0].get("scores", {}).get("hybrid", 0.0)
            })

    return similar_results

async def _find_similar_via_regex(self, cwe_id: str, limit: int = 5) -> List[Dict[str, Any]]:
    """Fallback: Regex-based discovery with conservative patterns."""

    # This is the fallback implementation, kept for compatibility
    # but with more conservative patterns
    # ... (implementation similar to original but with better filtering)
```

### Phase 2: Follow-up Question Processing (Estimated: 2-3 hours)

#### Step 2.1: Implement Follow-up Intent Detection (AC7)
**Enhancement**: Add follow-up processing to conversation.py

```python
# In conversation.py - New method
def detect_followup_intent(self, query: str, context: UserContext) -> Dict[str, Any]:
    """Detect if query is a follow-up question and extract intent."""

    followup_patterns = {
        "consequences": [r"what.*consequence", r"impact", r"effect"],
        "examples": [r"example", r"show.*me", r"demonstrate"],
        "mitigation": [r"how.*fix", r"prevent", r"mitigate", r"solution"],
        "relationships": [r"related.*cwe", r"similar", r"connect"],
        "detection": [r"how.*detect", r"find.*this", r"identify"]
    }

    query_lower = query.lower()

    # Check for follow-up indicators
    followup_indicators = ["what are", "how does", "give me", "show me", "can you"]
    is_followup = any(indicator in query_lower for indicator in followup_indicators)

    if is_followup and context.last_cwes_discussed:
        # Determine intent category
        for intent, patterns in followup_patterns.items():
            if any(re.search(pattern, query_lower) for pattern in patterns):
                return {
                    "is_followup": True,
                    "intent": intent,
                    "target_cwes": context.last_cwes_discussed[-3:],  # Last 3 CWEs
                    "original_query": query
                }

    return {"is_followup": False}

# Enhanced message processing
async def process_user_message_streaming(self, session_id: str, message_content: str, message_id: str):
    # ... existing code ...

    # Check for follow-up intent
    followup_info = self.detect_followup_intent(sanitized_query, context)

    if followup_info["is_followup"]:
        # Process as follow-up question
        retrieved_chunks = await self._process_followup_query(
            followup_info, context
        )
    else:
        # Process as new query (existing logic)
        retrieved_chunks = await self.query_handler.process_query(
            sanitized_query, context.get_persona_preferences()
        )
```

#### Step 2.2: Context-Aware Response Generation (AC8)
**Enhancement**: Ensure responses are extracted from stored CWE metadata

```python
# In response_generator.py - Enhanced fact verification
def verify_facts_against_source(self, response: str, source_chunks: List[Dict]) -> Dict[str, Any]:
    """Verify response facts against source CWE metadata."""

    # Extract claims from response
    claims = self._extract_factual_claims(response)

    verified_facts = []
    unverified_claims = []

    for claim in claims:
        # Check if claim can be verified against source chunks
        verification = self._verify_claim_against_chunks(claim, source_chunks)
        if verification["verified"]:
            verified_facts.append({
                "claim": claim,
                "source": verification["source_chunk"],
                "confidence": verification["confidence"]
            })
        else:
            unverified_claims.append(claim)

    return {
        "verified_facts": verified_facts,
        "unverified_claims": unverified_claims,
        "verification_score": len(verified_facts) / len(claims) if claims else 1.0
    }
```

### Phase 3: Progressive Disclosure UI (Estimated: 2 hours)

#### Step 3.1: Summary/Detail Response Formatting (AC10)
**Implementation**: Add progressive disclosure to response_generator.py

```python
# In response_generator.py - New formatting method
def format_progressive_response(self, full_response: str, persona: str) -> Dict[str, Any]:
    """Format response with summary and expandable details."""

    # Split response into summary and details
    lines = full_response.split('\n')

    # Extract summary (first 2-3 sentences or first section)
    summary_lines = []
    detail_lines = []

    in_summary = True
    sentence_count = 0

    for line in lines:
        if in_summary:
            summary_lines.append(line)
            # Count sentences in this line
            sentence_count += len([s for s in line.split('.') if s.strip()])

            # Switch to details after 3 sentences or if we hit a section header
            if sentence_count >= 3 or (line.startswith('#') and summary_lines):
                in_summary = False
        else:
            detail_lines.append(line)

    summary = '\n'.join(summary_lines[:3])  # Limit summary length
    details = '\n'.join(detail_lines)

    return {
        "summary": summary.strip(),
        "details": details.strip(),
        "has_more_details": bool(details.strip())
    }

# In main.py - Add action button for expanding details
@cl.action_callback("show_more_details")
async def on_show_more_details(action):
    """Handle show more details action."""
    # Get stored detailed response from session
    detailed_response = cl.user_session.get("last_detailed_response")

    if detailed_response:
        # Send full detailed response
        await cl.Message(
            content=detailed_response,
            author="Assistant"
        ).send()
    else:
        await cl.Message(
            content="No additional details available.",
            author="System"
        ).send()
```

### Phase 4: Confidence Scoring System (Estimated: 3 hours)

#### Step 4.1: Implement Confidence Score Calculation (AC14)
**Implementation**: Add confidence scoring to query_handler.py

```python
# In query_handler.py - New confidence calculation
def calculate_confidence_score(self, chunks: List[Dict[str, Any]], query: str) -> float:
    """Calculate confidence score based on retrieval results."""

    if not chunks:
        return 0.0

    # Factors for confidence calculation
    factors = {
        "similarity_score": 0.4,    # Vector similarity
        "result_count": 0.2,        # Number of relevant results
        "query_specificity": 0.2,   # How specific the query is
        "source_diversity": 0.2     # Diversity of sources
    }

    # Calculate similarity score (average of top 3 results)
    top_scores = [chunk.get("scores", {}).get("hybrid", 0.0) for chunk in chunks[:3]]
    avg_similarity = sum(top_scores) / len(top_scores) if top_scores else 0.0

    # Calculate result count factor (more results = higher confidence, up to a point)
    result_count_factor = min(len(chunks) / 5.0, 1.0)

    # Calculate query specificity (CWE IDs, technical terms)
    query_lower = query.lower()
    specificity_indicators = [
        r'cwe-\d+',           # Direct CWE references
        r'vulnerability',     # Security terms
        r'buffer overflow',   # Specific vulnerability types
        r'injection',
        r'authentication'
    ]

    specificity_score = sum(
        1 for pattern in specificity_indicators
        if re.search(pattern, query_lower)
    ) / len(specificity_indicators)

    # Calculate source diversity (different CWEs represented)
    unique_cwes = len(set(chunk["metadata"]["cwe_id"] for chunk in chunks))
    diversity_factor = min(unique_cwes / 3.0, 1.0)

    # Combine factors
    confidence = (
        factors["similarity_score"] * avg_similarity +
        factors["result_count"] * result_count_factor +
        factors["query_specificity"] * specificity_score +
        factors["source_diversity"] * diversity_factor
    )

    return min(max(confidence, 0.0), 1.0)  # Clamp to [0, 1]

# Enhanced process_query method
async def process_query(self, query: str, user_context: Dict[str, Any]) -> Dict[str, Any]:
    """Process query with confidence scoring."""

    chunks = await self._retrieve_chunks(query, user_context)
    confidence_score = self.calculate_confidence_score(chunks, query)

    return {
        "chunks": chunks,
        "confidence_score": confidence_score,
        "confidence_level": self._get_confidence_level(confidence_score)
    }

def _get_confidence_level(self, score: float) -> str:
    """Convert confidence score to human-readable level."""
    if score >= 0.8:
        return "High"
    elif score >= 0.6:
        return "Medium"
    elif score >= 0.4:
        return "Low"
    else:
        return "Very Low"
```

#### Step 4.2: Implement Low-Confidence Handling (AC15)
**Implementation**: Add low-confidence detection and suggestions

```python
# In conversation.py - Enhanced message processing
async def process_user_message_streaming(self, session_id: str, message_content: str, message_id: str):
    # ... existing retrieval logic ...

    result = await self.query_handler.process_query(sanitized_query, user_context_data)
    chunks = result["chunks"]
    confidence_score = result["confidence_score"]

    # Handle low confidence scenarios
    if confidence_score < 0.4:  # Low confidence threshold
        suggestions = self._generate_query_refinement_suggestions(
            sanitized_query, chunks, context.persona
        )

        # Create low-confidence response with suggestions
        low_confidence_msg = self._format_low_confidence_response(
            sanitized_query, confidence_score, suggestions, context.persona
        )

        msg = cl.Message(content=low_confidence_msg)
        await msg.send()

        return {
            "response": low_confidence_msg,
            "confidence_score": confidence_score,
            "suggestions": suggestions,
            "is_low_confidence": True
        }

    # Normal processing for adequate confidence
    # ... continue with response generation ...

def _generate_query_refinement_suggestions(self, query: str, chunks: List[Dict], persona: str) -> List[str]:
    """Generate suggestions to improve query for better results."""

    suggestions = []

    # Analyze what might be missing
    query_lower = query.lower()

    # Suggest more specific terms
    if not re.search(r'cwe-\d+', query_lower):
        suggestions.append("Try including a specific CWE ID (e.g., 'CWE-79' for XSS)")

    # Suggest persona-specific improvements
    persona_suggestions = {
        "Developer": [
            "Include specific programming language or framework",
            "Mention the type of vulnerability (injection, overflow, etc.)"
        ],
        "PSIRT Member": [
            "Include CVE ID if available",
            "Specify the affected product or component"
        ],
        "Bug Bounty Hunter": [
            "Describe the attack vector or exploitation method",
            "Include the vulnerable functionality or endpoint"
        ]
    }

    if persona in persona_suggestions:
        suggestions.extend(persona_suggestions[persona])

    # Generic improvements
    if len(query.split()) < 3:
        suggestions.append("Provide more context or details about the security issue")

    return suggestions[:3]  # Limit to top 3 suggestions

def _format_low_confidence_response(self, query: str, confidence: float, suggestions: List[str], persona: str) -> str:
    """Format response for low-confidence scenarios."""

    response = f"""I found some information related to your query, but my confidence is low ({confidence:.1%}).

**To get better results, try:**
"""

    for i, suggestion in enumerate(suggestions, 1):
        response += f"\n{i}. {suggestion}"

    if persona == "CVE Creator":
        response += "\n\n💡 **Tip**: CVE Creator works best with uploaded vulnerability reports or advisories. Try attaching a PDF with your research."

    response += "\n\nFeel free to rephrase your question with more specific details!"

    return response
```

### Phase 5: UI Enhancements (Estimated: 2 hours)

#### Step 5.1: Add Confidence Score Display
**Implementation**: Enhance main.py to show confidence scores

```python
# In main.py - Enhanced message processing with confidence display
async def main(message: cl.Message):
    # ... existing processing ...

    result = await conversation_manager.process_user_message_streaming(
        session_id=session_id,
        message_content=user_query,
        message_id=message.id
    )

    # Add confidence score display if available
    if result.get("confidence_score") is not None:
        confidence_score = result["confidence_score"]
        confidence_level = result.get("confidence_level", "Unknown")

        # Add confidence indicator to the response
        confidence_indicator = f"\n\n**Confidence**: {confidence_level} ({confidence_score:.1%})"

        # Update the message with confidence information
        if result.get("message"):
            current_content = result["message"].content
            result["message"].content = current_content + confidence_indicator
            await result["message"].update()

    # Add "Show More Details" action for summary responses
    if result.get("has_more_details"):
        actions = [
            cl.Action(
                name="show_more_details",
                value="show_more_details",
                label="Show More Details"
            )
        ]

        # Store detailed response for later use
        cl.user_session.set("last_detailed_response", result.get("full_response"))

        # Add actions to message
        if result.get("message"):
            result["message"].actions = actions
            await result["message"].update()
```

## Success Criteria

### Functional Requirements Completion

✅ **AC1: Flexible Input Patterns**
- InputSanitizer handles vulnerability descriptions, CVE advisories, tool outputs
- Multiple input format detection and preprocessing implemented

✅ **AC2: Secure File Upload**
- FileProcessor provides secure file handling with validation
- Supports PDF/text files up to 10MB with virus scanning

✅ **AC3: Data Privacy & Isolation**
- File content isolation using cl.user_session
- No data exfiltration from user's domain

✅ **AC4: Context Preservation**
- Session state management with UserContext in cl.user_session
- Conversation history preserved across sessions

✅ **AC5: Input Size Management**
- File size limits (10MB) and text length validation (2000 chars)
- Graceful error handling for oversized inputs

🎯 **AC6: Comprehensive CWE Retrieval** (Enhanced)
- Structured CWE metadata extraction with relationships and consequences
- Enhanced query_handler.py with comprehensive metadata support

🎯 **AC7: Follow-up Questions** (New Implementation)
- Follow-up intent detection and context-aware processing
- Enhanced conversation.py with follow-up handling

🎯 **AC8: Factual Accuracy** (Enhanced)
- Fact verification against source CWE metadata
- Enhanced response_generator.py with verification system

🎯 **AC9: Similar CWEs** (New Implementation)
- Related CWE discovery using corpus relationships
- New find_similar_cwes method in query_handler.py

🎯 **AC10: Progressive Disclosure** (New Implementation)
- Summary/detail response formatting with Chainlit actions
- Enhanced main.py with "Show More Details" functionality

✅ **AC11: Role Selection**
- Persona selection UI with 7 roles in top bar and settings
- Complete persona management system implemented

✅ **AC12: Role-Based Responses**
- Persona-specific preferences and response tailoring
- Different content prioritization per role

✅ **AC13: Hallucination Mitigation**
- Input sanitization and prompt injection protection
- Response validation with security checks

🎯 **AC14: Confidence Scoring** (New Implementation)
- Confidence score calculation from vector similarity and query specificity
- UI display of confidence levels with scores

🎯 **AC15: Low-Confidence Handling** (New Implementation)
- Low-confidence detection with query refinement suggestions
- Persona-specific improvement recommendations

## Verification Steps

### Functional Testing

#### Enhanced CWE Retrieval Verification
```bash
# Test comprehensive metadata retrieval
poetry run python -c "
from apps.chatbot.src.query_handler import CWEQueryHandler
import asyncio

async def test_comprehensive_retrieval():
    handler = CWEQueryHandler(DATABASE_URL, GEMINI_API_KEY)
    result = await handler.process_query_comprehensive('buffer overflow', {'persona': 'Developer'})

    # Verify comprehensive metadata is included
    assert 'comprehensive_metadata' in result[0]
    assert 'consequences' in result[0]['comprehensive_metadata']
    print('✅ Comprehensive metadata retrieval working')

asyncio.run(test_comprehensive_retrieval())
"
```

#### Follow-up Question Testing
```bash
# Test follow-up intent detection
poetry run pytest apps/chatbot/tests/test_followup_processing.py -v
```

#### Confidence Scoring Validation
```bash
# Test confidence score calculation
poetry run python -c "
from apps.chatbot.src.query_handler import CWEQueryHandler

handler = CWEQueryHandler(DATABASE_URL, GEMINI_API_KEY)

# Test high-confidence query
result = handler.calculate_confidence_score(
    [{'scores': {'hybrid': 0.9}, 'metadata': {'cwe_id': 'CWE-79'}}],
    'CWE-79 cross-site scripting'
)
assert result > 0.7, f'Expected high confidence, got {result}'

# Test low-confidence query
result = handler.calculate_confidence_score([], 'vague security question')
assert result < 0.3, f'Expected low confidence, got {result}'

print('✅ Confidence scoring working correctly')
"
```

### Integration Testing

#### End-to-End Follow-up Flow
```bash
# Test complete follow-up conversation flow
poetry run pytest apps/chatbot/tests/e2e/test_followup_conversation.py
```

#### Progressive Disclosure UI Testing
```bash
# Test summary/details UI with Playwright
poetry run pytest apps/chatbot/tests/e2e/test_progressive_disclosure.py
```

### Security Verification

#### Ship Blocker Validation
```bash
# Test data egress controls
poetry run python -c "
from apps.chatbot.src.config import DataEgressValidator

# Test no-egress mode
validator = DataEgressValidator('none')
assert not validator.validate_embedding_request('test content')
assert not validator.validate_llm_request('test content')

# Test embeddings-only mode
validator = DataEgressValidator('embeddings_only')
assert validator.validate_embedding_request('test content')
assert not validator.validate_llm_request('test content')

print('✅ Data egress controls working')
"
```

#### Evidence Isolation Testing
```bash
# Critical test: Evidence never appears in prompts
poetry run pytest apps/chatbot/tests/test_evidence_isolation.py -v

# Test that evidence is cleared after generation
poetry run pytest apps/chatbot/tests/test_evidence_cleanup.py -v
```

#### Enhanced Input Validation
```bash
# Verify enhanced input processing doesn't introduce vulnerabilities
poetry run pytest apps/chatbot/tests/test_enhanced_input_security.py
```

#### Cross-Session Isolation Validation
```bash
# Automated test: spawn two sessions, assert no context bleed
poetry run pytest apps/chatbot/tests/test_session_isolation.py -v
```

### Manual Verification

#### Complete User Journey Testing
1. **Start new session** → Select "Developer" persona
2. **Ask initial question** → "What is buffer overflow?"
3. **Verify confidence score displayed** → Should show High confidence
4. **Ask follow-up** → "What are its consequences?"
5. **Verify context preservation** → Response should reference buffer overflow
6. **Test progressive disclosure** → Click "Show More Details" if available
7. **Test low confidence** → Ask "security things" (vague query)
8. **Verify suggestions** → Should provide query refinement suggestions

#### Role-Based Response Verification
1. **Switch to PSIRT Member** persona
2. **Ask same buffer overflow question**
3. **Verify different response focus** → Should emphasize impact and advisory language
4. **Compare with Developer response** → Should show different priorities

## Time Estimation

### Revised Development Time Breakdown (Based on Plan Review)
- **Phase 0** (Foundation & Ship Blockers): 4 hours
- **Phase 1** (Enhanced CWE Retrieval + Caching): 3 hours
- **Phase 2** (Conservative Follow-up Detection): 2 hours
- **Phase 3** (Progressive Disclosure UI): 1.5 hours
- **Phase 4** (Confidence Scoring + Feature Flags): 2 hours
- **Phase 5** (Deferred): Virus scanning, GCS storage, PII masking

**Total Core Implementation**: 12.5 hours

### Critical Path (Revised)
1. **Phase 0**: Feature flags and security foundation (blocks everything)
2. **Phase 1**: Enhanced retrieval with relationship-based similar CWEs
3. **Phase 4**: Confidence scoring (enables UI display)
4. **Phase 2**: Conservative follow-up detection
5. **Phase 3**: Progressive disclosure (final integration)

### Dependencies
- **Story 1.5** production infrastructure must remain stable
- **Gemini API** quota sufficient for enhanced retrieval testing
- **Database access** for comprehensive metadata queries
- **Feature flag system** for safe deployment

### Deferred to Future Stories
- **Virus scanning implementation** (interface scaffolding only)
- **GCS storage encryption** (local file handling for now)
- **Advanced PII masking** (basic secrets redaction implemented)
- **Cross-session persistence** (single-session scope)

### Buffer Time
- **Additional 2-3 hours** for integration testing and refinement
- **1 hour** for golden test creation and validation

**Total Implementation Time**: 15-16 hours

## Risk Mitigation

### Technical Risks

**Risk**: Enhanced retrieval affects performance
- **Mitigation**: Implement caching for comprehensive metadata queries
- **Monitoring**: Track query response times, maintain <200ms p95 target

**Risk**: Confidence scoring algorithm produces inaccurate scores
- **Mitigation**: Extensive testing with known queries and manual validation
- **Rollback**: Configurable confidence threshold and ability to disable display

**Risk**: Follow-up intent detection has false positives
- **Mitigation**: Conservative detection patterns, manual testing with various queries
- **Fallback**: Graceful degradation to normal query processing

### Dependency Risks

**Risk**: Story 1.5 infrastructure changes break integration
- **Mitigation**: Version pinning and comprehensive integration tests
- **Communication**: Coordinate with infrastructure team on any changes

**Risk**: Chainlit UI framework limitations for progressive disclosure
- **Mitigation**: Prototype UI enhancements early, have fallback designs
- **Alternative**: Use markdown expansion instead of action buttons if needed

### Resource Constraints

**Risk**: Gemini API quota exceeded during testing
- **Mitigation**: Implement request caching, use test database with limited data
- **Monitoring**: Track API usage and implement rate limiting

### Timeline Risks

**Risk**: Implementation takes longer than estimated
- **Mitigation**: Implement features incrementally, prioritize by acceptance criteria importance
- **Scope Adjustment**: Progressive disclosure and confidence scoring can be delayed if needed

### Rollback Plans

**Graceful Degradation Strategy**:
1. **Enhanced Retrieval Issues** → Fall back to existing basic retrieval
2. **Follow-up Detection Problems** → Disable follow-up processing, use normal query handling
3. **Confidence Scoring Errors** → Hide confidence display, continue with normal responses
4. **UI Enhancement Failures** → Remove action buttons, use plain text responses

**Rollback Validation**:
- All new features have feature flags for quick disable
- Existing functionality must continue working if new features are disabled
- Comprehensive test suite validates rollback scenarios

## Next Steps

### Immediate Follow-up Work
1. **Story 3.2**: Advanced Search Filters and Query Refinement
   - Build on confidence scoring to add guided search refinement
   - Implement advanced filtering by CWE categories, severity, affected platforms

2. **Story 3.3**: Conversation Analytics and User Insights
   - Leverage enhanced context preservation for user behavior analysis
   - Implement conversation quality metrics using confidence scores

### Integration Points
1. **Story 4.x**: Multi-modal Input Support
   - Build on file upload infrastructure for image and diagram processing
   - Extend progressive disclosure for visual content

2. **Story 5.x**: Collaborative Features
   - Use session context preservation for team-based vulnerability analysis
   - Extend role-based responses for team collaboration

### Monitoring Requirements
1. **Confidence Score Distribution**: Track accuracy of confidence predictions
2. **Follow-up Usage Patterns**: Monitor follow-up question effectiveness
3. **Progressive Disclosure Engagement**: Measure "Show More Details" usage
4. **Query Refinement Success**: Track improvement after low-confidence suggestions

### Documentation Updates
1. **User Guide**: Add sections on follow-up questions and progressive disclosure
2. **Developer Documentation**: Document new confidence scoring and retrieval APIs
3. **Architecture Documentation**: Update with enhanced conversation flow diagrams
4. **Security Documentation**: Review and update with new input processing features

## Implementation Readiness Checklist

### Pre-Implementation Ship Blockers
- [ ] **Data Egress Mode**: Implement DATA_EGRESS_MODE configuration with runtime enforcement
- [ ] **Evidence Isolation**: Ensure evidence never concatenated to prompts; injected only as low-weight EVIDENCE chunk
- [ ] **Evidence Cleanup**: Evidence cleared from session after generation (all code paths)
- [ ] **Feature Flags**: Implement feature flags for safe deployment and rollback

### Development Phase Gates
- [ ] **Phase 0 Complete**: Feature flags, type contracts, evidence cleanup guards implemented
- [ ] **Phase 1 Complete**: Enhanced retrieval with relationship-based similar CWEs
- [ ] **Phase 2 Complete**: Conservative follow-up detection with fallback safety
- [ ] **Phase 3 Complete**: Progressive disclosure stores details server-side; no evidence echoing
- [ ] **Phase 4 Complete**: Confidence scoring with display behind flag; golden tests added

### Security Validation Gates
- [ ] **Evidence Never in Prompts**: Evidence never concatenated to prompts; injected only as low-weight EVIDENCE chunk
- [ ] **Cross-Session Isolation**: Automated test confirms no context bleeding between sessions
- [ ] **Secrets Redaction**: Control-char stripping from evidence; secrets redaction in logs
- [ ] **Data Egress Enforcement**: DATA_EGRESS_MODE enforced with comprehensive tests

### Quality Gates
- [ ] **Type Safety**: Pydantic/typing added where relevant; strongly-typed contracts implemented
- [ ] **Golden Tests**: Confidence score contract with expected score bands for known queries
- [ ] **Conservative Detection**: Follow-up detection conservative; graceful fallback to normal processing
- [ ] **Performance**: Enhanced retrieval maintains <200ms p95 target with caching

### Testing Completeness
- [ ] **Unit Tests**: New unit tests passing for all enhanced functionality
- [ ] **Integration Tests**: Enhanced functionality integrates properly with existing components
- [ ] **E2E Tests**: Playwright tests for progressive disclosure and confidence display
- [ ] **Security Tests**: Evidence isolation, session isolation, data egress validation

This implementation plan provides a comprehensive roadmap for completing Story 3.1 while addressing all ship blockers identified in the plan review. The focus is on secure, maintainable enhancement of existing functionality with proper feature flags and validation.