# Epic 2.3 Implementation Plan: Role-Based Context Awareness & Hallucination Mitigation

## Overview

This plan implements role-based context awareness and hallucination mitigation for the CWE ChatBot. The implementation will add user role selection, role-based prompt templating, confidence scoring, and citation mechanisms to provide accurate, tailored responses while maintaining security integrity.

## Pre-Implementation Checklist

### Prerequisites Verification
- [x] Story 2.3 approved and requirements defined
- [ ] Chainlit application running (legacy HybridRAGManager removed; use ingestion-aligned retrieval via `CWEQueryHandler`)
- [ ] Vector database populated with CWE corpus data
- [ ] LLM integration functional (embedding service, response generation)
- [ ] Session management components operational from Story 2.2

### Tool/Environment Requirements
- [ ] Python 3.10+ environment with Poetry
- [ ] Chainlit framework (latest stable version)
- [ ] PostgreSQL database access for session management
- [ ] Vector database (current implementation) access
- [ ] Testing framework (pytest) configured
- [ ] Linting tools (black, ruff) available

### Access Requirements
- [ ] Write access to `apps/chatbot/src/` directory structure
- [ ] Database connection for session storage validation
- [ ] Vector database access for confidence score retrieval
- [ ] LLM API access for prompt template testing

### Architecture Dependencies
- [ ] Existing SessionManager from Story 2.2 functional
- [ ] Update confidence display with ingestion RRF scores (hybrid/vec/fts/alias)
- [ ] Response formatting system operational
- [ ] Security components (rate limiting, logging) functional

### Security Considerations
- [ ] Session integrity mechanisms in place
- [ ] Role data cannot be manipulated via user input
- [ ] CSRF protection for role selection actions
- [ ] Input validation for all role-related operations
- [ ] Secure logging configured for role changes

## Implementation Steps

### Phase 1: Role Management Infrastructure (Estimated: 90 minutes)

#### Step 1.1: Create Role Manager Module (30 minutes)
```python
# File: apps/chatbot/src/user/role_manager.py
class UserRole(Enum):
    PSIRT = "psirt"
    DEVELOPER = "developer" 
    ACADEMIC = "academic"
    BUG_BOUNTY = "bug_bounty"
    PRODUCT_MANAGER = "product_manager"

class RoleManager:
    def set_user_role(self, role: UserRole) -> bool
    def get_current_role(self) -> Optional[UserRole]
    def get_role_actions(self) -> list
    def validate_role_integrity(self) -> bool
```

#### Step 1.2: Create Role Manager Tests (30 minutes)
```bash
# Test file: apps/chatbot/tests/test_role_manager.py
poetry run pytest apps/chatbot/tests/test_role_manager.py -v
```

#### Step 1.3: Integrate Role Selection UI (30 minutes)
- Modify `@cl.on_chat_start` to show role selection on first visit
- Add role selection actions to welcome message
- Implement role selection callback handlers

### Phase 2: Role-Based Prompt Templating (Estimated: 120 minutes)

#### Step 2.1: Create Prompt Template System (45 minutes)
```python
# File: apps/chatbot/src/prompts/role_templates.py
class RolePromptTemplates:
    def get_role_prompt(self, role: UserRole, context: dict) -> str
    def _get_psirt_prompt_template(self) -> str
    def _get_developer_prompt_template(self) -> str
```

#### Step 2.2: Implement Role Context Integration (45 minutes)
- Modify query processing to include role context
- Update LLM prompt generation with role-specific templates
- Integrate with ingestion-aligned retrieval (`PostgresChunkStore.query_hybrid`)

#### Step 2.3: Test Role-Based Responses (30 minutes)
```bash
# Manual testing script
python apps/chatbot/tests/manual/test_role_responses.py
```

### Phase 3: Confidence Scoring Enhancement (Estimated: 75 minutes)

#### Step 3.1: Enhance Confidence Score Display (30 minutes)
- Modify response formatter to show user-friendly confidence percentages
- Normalize vector similarity scores to percentage ranges
- Add confidence indicators to UI responses

#### Step 3.2: Implement Confidence-Based Response Logic (30 minutes)
```python
# In Response Generator or UI layer (legacy components removed)
def get_confidence_percentage(self, similarity_score: float) -> int:
    return min(100, max(10, int(similarity_score * 100)))
```

#### Step 3.3: Test Confidence Display (15 minutes)
- Verify confidence scores appear in UI
- Test with various query types and confidence levels

### Phase 4: Citation and Low-Confidence Handling (Estimated: 105 minutes)

#### Step 4.1: Implement Citation System (45 minutes)
```python
# File: apps/chatbot/src/processing/citation_handler.py
class CitationHandler:
    def add_citations_to_response(self, response: str, sources: list) -> str
    def format_citation(self, source: CWEResult) -> str
```

#### Step 4.2: Low-Confidence Response Handler (45 minutes)
- Define confidence threshold (default: 70%)
- Implement low-confidence response generation
- Add suggestions for query refinement

#### Step 4.3: Integration Testing (15 minutes)
- Test citation display with various queries
- Test low-confidence handling with ambiguous inputs

## Success Criteria

✅ **Acceptance Criteria Mapping:**

- **AC1**: Role selection UI appears on session start with 5 role options
- **AC2**: Different roles produce observably different response content/emphasis
- **AC3**: Responses include citations from CWE corpus, flag derived information
- **AC4**: Confidence scores displayed alongside CWE suggestions (percentage format)
- **AC5**: Low confidence triggers explicit limitation statements and refinement suggestions

## Verification Steps

### Functional Testing
1. **Role Selection Testing**
   ```bash
   # Start fresh session, verify role selection appears
   poetry run chainlit run apps/chatbot/main.py
   # Manual: Click through each role option, verify session storage
   ```

2. **Role-Based Response Testing**
   ```bash
   # Test script for role response differences
   python apps/chatbot/tests/manual/verify_role_responses.py
   ```

3. **Confidence Scoring Testing**
   ```bash
   # Verify confidence percentages appear in responses
   poetry run pytest apps/chatbot/tests/test_confidence_display.py -v
   ```

4. **Citation Testing**
   ```bash
   # Verify citations appear in responses
   poetry run pytest apps/chatbot/tests/test_citation_system.py -v
   ```

### Security Validation
1. **Role Integrity Testing**
   ```python
   # Test that role cannot be manipulated via user input
   def test_role_manipulation_protection():
       # Attempt to inject malicious role values
       # Verify role validation prevents tampering
   ```

2. **Session Security Testing**
   ```bash
   # Verify role data is securely stored in session
   poetry run pytest apps/chatbot/tests/test_role_security.py -v
   ```

### Integration Testing
1. **Full Pipeline Testing**
   ```bash
   # End-to-end test with role selection → query → role-based response
   poetry run pytest apps/chatbot/tests/integration/test_role_based_pipeline.py -v
   ```

### Manual Verification (from Story Requirements)
1. Start session → select "Developer" → ask about "CWE-89" → note response structure
2. Start session → select "PSIRT" → ask about "CWE-89" → verify different emphasis
3. Submit ambiguous query → verify low confidence handling and refinement suggestions

## Time Estimation

- **Total Estimated Time**: 6.5 hours (390 minutes)
- **Critical Path**: Role Management → Prompt Templates → Integration Testing
- **Dependencies**: Existing session management, vector DB access, LLM integration
- **Buffer Time**: +2 hours for unexpected integration issues (25% buffer)

**Realistic Total**: 8.5 hours including testing and debugging

## Risk Mitigation

### Technical Risks
1. **Risk**: Role-based prompts may not produce sufficiently different responses
   - **Mitigation**: Create distinct, well-tested prompt templates with clear role-specific instructions
   - **Rollback**: Fall back to generic prompts if role differentiation fails

2. **Risk**: Confidence scoring normalization may not align with user expectations
   - **Mitigation**: Test with various similarity score ranges, adjust normalization algorithm
   - **Rollback**: Display raw similarity scores if percentage conversion is problematic

3. **Risk**: Citation extraction may be complex with existing response format
   - **Mitigation**: Start with simple source attribution, enhance iteratively
   - **Rollback**: Implement basic "Source: CWE-XXX" format as minimum viable citation

### Dependency Risks
1. **Risk**: Session management from Story 2.2 may have integration issues
   - **Mitigation**: Validate session manager functionality before role integration
   - **Rollback**: Implement temporary in-memory role storage if session issues occur

2. **Risk**: Vector database confidence scores may not map well to percentages
   - **Mitigation**: Analyze existing score distributions before implementing normalization
   - **Rollback**: Use qualitative confidence levels (High/Medium/Low) instead of percentages

### Timeline Risks
1. **Risk**: Role-based prompt engineering may require extensive iteration
   - **Mitigation**: Start with simple role distinctions, refine based on testing
   - **Rollback**: Implement basic role tags in responses rather than full prompt customization

### Rollback Plans
- **Complete Rollback**: All role-based features can be disabled via feature flags
- **Partial Rollback**: Individual components (role UI, prompts, confidence, citations) can be independently disabled
- **Data Safety**: Role selection uses session storage only, no permanent data changes

## Next Steps

After Story 2.3 completion:

1. **Enhanced Role Customization**: Allow users to customize role preferences within selected role
2. **Role-Based Analytics**: Track role selection patterns and response effectiveness  
3. **Advanced Citation System**: Implement clickable citations with source highlighting
4. **Confidence Calibration**: Use user feedback to improve confidence score accuracy
5. **Cross-Role Learning**: Analyze how different roles interact with same CWEs for insights

## Integration Points

- **Story 2.2 Dependencies**: Session management, progressive disclosure system
- **Story 2.4 Preparation**: Role context will enhance future advanced features
- **Security Story Integration**: Role integrity supports overall session security model
- **Testing Infrastructure**: Role-based tests will support ongoing regression testing

## Monitoring & Maintenance

- **Role Selection Metrics**: Track role distribution and selection patterns
- **Response Quality Monitoring**: Monitor role-based response effectiveness
- **Confidence Score Calibration**: Regular validation of confidence score accuracy
- **Citation System Health**: Ensure citation extraction remains accurate with CWE corpus updates
