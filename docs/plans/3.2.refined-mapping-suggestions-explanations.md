# Epic 3.2 Implementation Plan: Refined Mapping Suggestions & Explanations

## Overview

Story 3.2 aims to enhance the CWE ChatBot with precise, explained CWE mapping suggestions. This will provide users with prioritized CWE recommendations, confidence scores, clear reasoning explanations, and CWE relationship exploration capabilities. The implementation will minimize new code by building on existing Story 2.1 RAG infrastructure while adding confidence scoring, result prioritization, and relationship exploration features.

## Analysis of Existing Implementation

### ðŸŸ¢ Already Implemented (Reusable Components)

#### Core RAG Infrastructure (Story 2.1)
- **Query Handler** (`query_handler.py`): Production-ready hybrid retrieval system
  - PostgreSQL + vector database integration
  - Gemini embeddings (3072D)
  - Hybrid scoring (vector + FTS + alias matching)
  - Already returns scored results with metadata
- **Response Generator** (`response_generator.py`): Persona-specific response generation
  - Role-based prompting (Developer, PSIRT, etc.)
  - Context building from retrieved chunks
  - Source attribution and CWE references
- **Vector Database**: Production CWE corpus (969 CWEs, 7,913 chunks with 14 semantic sections)
- **Query Processor** (`query_processor.py`):
  - CWE extraction and enhancement
  - Security-first input processing
  - Query type classification

#### Testing Infrastructure
- **E2E Tests**: Comprehensive retrieval testing with real data
- **Unit Tests**: Component validation for core functionality
- **Integration Tests**: Role-specific response verification

### ðŸŸ¡ Partially Implemented (Needs Enhancement)

#### Confidence Scoring
- **Current**: Basic hybrid scores from retrieval (vector + FTS + alias)
- **Gap**: No normalized confidence scores or thresholds for AC1

#### CWE Filtering
- **Current**: No filtering of Prohibited/Discouraged CWEs
- **Gap**: Need filtering mechanism for AC2

#### Explanation Generation
- **Current**: Basic context building, minimal reasoning
- **Gap**: Need clear explanations with CWE snippet citations for AC3

#### Relationship Exploration
- **Current**: Chunks contain relationship data but no structured exploration
- **Gap**: Need interactive relationship navigation for AC4

#### Low-Confidence Guidance
- **Current**: Fallback responses for no results
- **Gap**: No specific guidance for improving queries when confidence is low (AC5)

## Implementation Plan

### Phase 1: Core Processing Modules (Estimated: 4 hours)

#### Step 1.1: Confidence Calculator with Production Interface
```python
# apps/chatbot/src/processing/confidence_calculator.py
from __future__ import annotations
from typing import TypedDict, Literal, Iterable
import math

ConfidenceLevel = Literal["High", "Medium", "Low", "Very Low"]

class AggregatedCWE(TypedDict):
    cwe_id: str
    name: str
    top_hybrid_scores: list[float]     # top N hybrid scores from retrieval
    exact_alias_match: bool            # name/alias exact hit
    section_hits: dict[str, int]       # {"Description":2,"Consequences":1,...}
    source_count: int                  # distinct chunks/sources

class ConfidenceCalculator:
    def __init__(self,
                 w_sim: float = 0.55,
                 w_count: float = 0.15,
                 w_alias: float = 0.15,
                 w_sections: float = 0.1,
                 w_diversity: float = 0.05):
        self.w_sim, self.w_count, self.w_alias, self.w_sections, self.w_diversity = \
            w_sim, w_count, w_alias, w_sections, w_diversity

    @staticmethod
    def _norm_sim(scores: Iterable[float]) -> float:
        if not scores: return 0.0
        top = list(scores)[:3]
        avg = sum(top)/len(top)
        return 1/(1+math.exp(-8*(avg-0.6))) if avg>1 else max(0.0, min(1.0, avg))

    def score(self, agg: AggregatedCWE) -> float:
        # Multi-factor confidence calculation
        sim = self._norm_sim(agg["top_hybrid_scores"])
        count = min(len(agg["top_hybrid_scores"])/5.0, 1.0)
        alias = 1.0 if agg["exact_alias_match"] else 0.0
        sections = min(sum(1 for k,v in agg["section_hits"].items() if v>0)/4.0, 1.0)
        diversity = min(agg["source_count"]/4.0, 1.0)
        raw = (self.w_sim*sim + self.w_count*count + self.w_alias*alias +
               self.w_sections*sections + self.w_diversity*diversity)
        return max(0.0, min(1.0, raw))

    @staticmethod
    def level(score: float) -> ConfidenceLevel:
        if score >= 0.80: return "High"
        if score >= 0.60: return "Medium"
        if score >= 0.40: return "Low"
        return "Very Low"
```

#### Step 1.2: CWE Filter with Hard Limits
```python
# apps/chatbot/src/processing/cwe_filter.py
from __future__ import annotations
from typing import Iterable

MAX_RECS = 5

class CWEFilter:
    def __init__(self, prohibited: set[str] | None = None, discouraged: set[str] | None = None):
        self.prohibited = {c.upper() for c in (prohibited or set())}
        self.discouraged = {c.upper() for c in (discouraged or set())}

    def filter(self, recs: list[dict]) -> list[dict]:
        out = []
        for r in recs:
            cid = r["cwe_id"].upper()
            if cid in self.prohibited:
                r["filter_reason"] = "prohibited"; continue
            if cid in self.discouraged:
                r["filter_reason"] = "discouraged"; continue
            out.append(r)
        return out[:MAX_RECS]  # Hard cap
```

#### Step 1.3: Explanation Builder with Citation
```python
# apps/chatbot/src/processing/explanation_builder.py
from __future__ import annotations
from typing import List, Dict

SECTION_PRIORITY = ["Description","Common Consequences","Detection Methods","Mitigations","Relationships"]

def _short_quote(text: str, max_chars: int = 240) -> str:
    t = " ".join(text.split())
    return t[:max_chars] + ("â€¦" if len(t) > max_chars else "")

class ExplanationBuilder:
    def build(self, query: str, cwe_id: str, chunks: List[Dict]) -> Dict:
        ranked = sorted(chunks, key=lambda c: (
            -SECTION_PRIORITY.index(c["metadata"].get("section","Description"))
            if c["metadata"].get("section") in SECTION_PRIORITY else 99
        ))
        snippets = []
        seen_sections = set()
        for c in ranked:
            sec = c["metadata"].get("section","")
            if sec in seen_sections: continue
            snippets.append({
                "text": _short_quote(c["document"]),
                "section": sec,
                "citation": {"cwe_id": cwe_id, "section": sec, "chunk_id": c["metadata"].get("chunk_id")}
            })
            seen_sections.add(sec)
            if len(snippets) >= 2: break

        rationale_bullets = ["High semantic match to your query."]
        if any(s["section"]=="Description" for s in snippets):
            rationale_bullets.append("Description and consequences align with your input.")
        return {"snippets": snippets, "bullets": rationale_bullets}
```

### Phase 2: Relationship Infrastructure and Query Suggestions (Estimated: 3 hours)

#### Step 2.1: Relationship Repository Stub
```python
# apps/chatbot/src/repositories/relationship_repo.py
from __future__ import annotations
from typing import Dict, List

class RelationshipRepo:
    """Minimal stub for relationship lookup. Replace with DB-backed repo when available."""
    async def get_relationships(self, cwe_id: str) -> Dict[str, List[dict]]:
        # Returns: {"ParentOf": [...], "ChildOf": [...], "RelatedTo": [...]}
        sample = {
            "CWE-79": {
                "ParentOf": [],
                "ChildOf": [],
                "RelatedTo": [
                    {"cwe_id": "CWE-116", "name": "Improper Encoding or Escaping of Output"},
                    {"cwe_id": "CWE-20", "name": "Improper Input Validation"},
                ],
            }
        }
        return sample.get(cwe_id.upper(), {"ParentOf": [], "ChildOf": [], "RelatedTo": []})
```

#### Step 2.2: Relationship Parser
```python
# apps/chatbot/src/processing/relationship_parser.py
from __future__ import annotations
from typing import List, Dict

class RelationshipParser:
    def __init__(self, repo):
        self.repo = repo

    async def related(self, cwe_id: str) -> Dict[str, List[Dict]]:
        data = await self.repo.get_relationships(cwe_id)
        return data or {"ParentOf": [], "ChildOf": [], "RelatedTo": []}
```

#### Step 2.3: Query Suggester with Persona Hints
```python
# apps/chatbot/src/processing/query_suggester.py
from __future__ import annotations
from typing import List

PERSONA_HINTS = {
    "Developer": ["Add language/framework (e.g., 'in Spring Boot').",
                  "Include the failing API/endpoint or function name."],
    "PSIRT Member": ["Provide affected product/version.", "Add CVE/CWE if known."],
    "Bug Bounty Hunter": ["Describe the entry point and observed impact.",
                          "Include repro steps or payload shape."]
}

class QuerySuggester:
    def suggest(self, query: str, persona: str) -> List[str]:
        out = []
        if "cwe-" not in query.lower():
            out.append("Include a specific CWE if you suspect one (e.g., 'CWE-79').")
        if len(query.split()) < 6:
            out.append("Add a brief symptom or error message to anchor the context.")
        out.extend(PERSONA_HINTS.get(persona, [])[:2])
        return out[:3]
```

### Phase 3: Core Query Handler Integration (Estimated: 4 hours)

#### Step 3.1: Update CWEQueryHandler with Unified Pipeline
```python
# Key excerpts for apps/chatbot/src/query_handler.py
from typing import TypedDict, Literal
from .processing.confidence_calculator import ConfidenceCalculator
from .processing.cwe_filter import CWEFilter
from .processing.explanation_builder import ExplanationBuilder
from .processing.relationship_parser import RelationshipParser

class Recommendation(TypedDict):
    cwe_id: str
    name: str
    confidence: float
    level: Literal["High","Medium","Low","Very Low"]
    explanation: dict        # {"snippets":[{text,section,citation}], "bullets":[...]}
    top_chunks: list[dict]   # server-side only
    relationships: dict | None

class CWEQueryHandler:
    def __init__(self, store, embedder, relationship_repo, prohibited: set[str], discouraged: set[str]):
        self.store, self.embedder = store, embedder
        self.conf_calc = ConfidenceCalculator()
        self.filter = CWEFilter(prohibited, discouraged)
        self.explainer = ExplanationBuilder()
        self.rels = RelationshipParser(relationship_repo)

    async def process_query(self, query: str, user_ctx: dict) -> dict:
        # Pipeline: retrieve â†’ aggregate by CWE â†’ score â†’ explain â†’ filter â†’ sort
        raw_chunks = await self._retrieve_chunks(query, user_ctx)
        by_cwe = {}
        for ch in raw_chunks:
            cid = ch["metadata"]["cwe_id"]
            by_cwe.setdefault(cid, {"name": ch["metadata"].get("cwe_name", cid),
                                    "scores": [], "section_hits": {}, "chunks":[]})
            by_cwe[cid]["scores"].append(ch.get("scores",{}).get("hybrid",0.0))
            sec = ch["metadata"].get("section","")
            by_cwe[cid]["section_hits"][sec] = by_cwe[cid]["section_hits"].get(sec,0)+1
            by_cwe[cid]["chunks"].append(ch)

        recs: list[Recommendation] = []
        for cid, agg in by_cwe.items():
            # Build AggregatedCWE and calculate confidence
            conf = self.conf_calc.score(agg_model)
            level = self.conf_calc.level(conf)
            expl = self.explainer.build(query, cid, agg["chunks"])
            recs.append({
                "cwe_id": cid, "name": agg["name"],
                "confidence": conf, "level": level,
                "explanation": expl, "top_chunks": agg["chunks"],
                "relationships": None
            })

        # Sort by confidence, filter, and cap
        recs.sort(key=lambda r: r["confidence"], reverse=True)
        recs = self.filter.filter(recs)

        return {
            "recommendations": recs,
            "low_confidence": (len(recs)==0 or (recs and recs[0]["confidence"] < 0.40))
        }

    async def get_relationships(self, cwe_id: str) -> dict:
        return await self.rels.related(cwe_id)
```

### Phase 4: Response Rendering and UI Actions (Estimated: 3 hours)

#### Step 4.1: Add MappingRenderer to ResponseGenerator
```python
# Add to apps/chatbot/src/response_generator.py
import re

def _sanitize_for_md(text: str) -> str:
    if not text: return ""
    t = " ".join(text.split())
    t = re.sub(r"[\x00-\x08\x0B\x0C\x0E-\x1F]", "", t)  # strip control chars
    return t.replace("```", "Ê¼Ê¼Ê¼")  # neutralize triple backticks

class MappingRenderer:
    @staticmethod
    def render(recommendations: List[Dict[str, Any]], persona: str) -> Dict[str, Any]:
        lines: List[str] = []
        actions: List[Dict[str, str]] = []

        if not recommendations:
            return {"markdown": "_No strong CWE matches found._", "actions": []}

        for i, r in enumerate(recommendations, 1):
            cwe_id = r["cwe_id"]
            name = r.get("name", cwe_id)
            conf = r.get("confidence", 0.0)
            level = r.get("level", "Low")
            lines.append(f"**{i}) {cwe_id} â€” {name}**  \nConfidence: **{conf:.2f}** ({level})")

            expl = r.get("explanation", {})
            bullets = expl.get("bullets", [])
            if bullets:
                lines.append("Why: " + "; ".join(_sanitize_for_md(b) for b in bullets))

            for snip in (expl.get("snippets") or [])[:2]:
                quote = _sanitize_for_md(snip.get("text", ""))
                section = snip.get("section") or "Source"
                lines.append(f"> {quote}  \nâ€” *{section}*, {cwe_id}")

            actions.append({
                "name": "explore_relationships",
                "value": cwe_id,
                "label": f"Explore {cwe_id} relationships"
            })
            lines.append("")

        return {"markdown": "\n".join(lines).strip(), "actions": actions}
```

#### Step 4.2: Update ResponseGenerator.build_response()
```python
# Modify ResponseGenerator class to handle recommendations
def build_response(self, persona: str, recommendations: List[Dict[str, Any]], **kwargs) -> Dict[str, Any]:
    if recommendations:
        bundle = self.mapping_renderer.render(recommendations, persona)
        return {
            "content": bundle["markdown"],
            "actions": bundle["actions"],
            "meta": {"has_recommendations": True}
        }

    # Fallback to existing response path
    text = "I couldn't produce mapping suggestions from the current context."
    return {"content": text, "actions": [], "meta": {"has_recommendations": False}}
```

#### Step 4.3: Wire Chainlit Actions in main.py
```python
# Add to apps/chatbot/src/main.py
from .repositories.relationship_repo import RelationshipRepo

# Bootstrap
relationship_repo = RelationshipRepo()
query_handler = CWEQueryHandler(store, embedder, relationship_repo, prohibited_cwes, discouraged_cwes)

@cl.on_message
async def on_message(message: cl.Message):
    # Existing conversation pipeline
    result = await conversation_manager.process_user_message_streaming(...)

    # Render recommendations with actions
    recs = result.get("recommendations") or []
    rendered = response_generator.build_response(persona=persona, recommendations=recs)

    msg = cl.Message(content=rendered["content"])
    if rendered.get("actions"):
        msg.actions = [cl.Action(name=a["name"], value=a["value"], label=a["label"])
                       for a in rendered["actions"]]
    await msg.send()

@cl.action_callback("explore_relationships")
async def on_explore_relationships(action: cl.Action):
    cwe_id = str(action.value).upper()
    rels = await query_handler.get_relationships(cwe_id)

    def _fmt_group(title: str, items: list[dict]) -> str:
        if not items: return f"- *None*"
        return "\n".join(f"- **{it['cwe_id']}** â€” {it.get('name', '')}".rstrip()
                         for it in items)

    content = (
        f"### Relationships for **{cwe_id}**\n\n"
        f"**ParentOf**\n{_fmt_group('ParentOf', rels.get('ParentOf', []))}\n\n"
        f"**ChildOf**\n{_fmt_group('ChildOf', rels.get('ChildOf', []))}\n\n"
        f"**RelatedTo**\n{_fmt_group('RelatedTo', rels.get('RelatedTo', []))}"
    )
    await cl.Message(content=content).send()
```

### Phase 5: Testing and Validation (Estimated: 2 hours)

#### Step 5.1: Unit Test Suite
Create comprehensive unit tests as provided in feedback:
- `test_mapping_renderer.py`: Confidence formatting, sanitization, action generation
- `test_relationship_repo.py`: Stub functionality and return format validation
- `test_confidence_calculator.py`: Score calculation and level classification
- `test_cwe_filter.py`: Prohibited filtering and result capping
- `test_explanation_builder.py`: Snippet extraction and citation formatting

#### Step 5.2: Integration Testing
- End-to-end pipeline: query â†’ 3-5 prioritized recommendations
- Action flow: button click â†’ relationship display
- Low-confidence guidance with persona-specific hints
- Security validation: no internal data leakage

#### Step 5.3: UI and Performance Validation
- Chainlit UI displays confidence scores and explanations correctly
- Relationship exploration works without errors
- Response times remain under 5 seconds
- Rate limiting prevents relationship query spam

## Success Criteria

### AC1: Concise Prioritized Recommendations with Confidence Scores âœ…
- [ ] Maximum 5 CWE recommendations per response
- [ ] Each recommendation includes confidence score (0-1 scale)
- [ ] Results ordered by confidence level
- [ ] Clear confidence indicators in UI ("High confidence", "Medium confidence", "Low confidence")

### AC2: Intelligent CWE Limiting and Filtering âœ…
- [ ] Prohibited/Discouraged CWEs filtered out automatically
- [ ] Maximum 5 suggestions to avoid information overload
- [ ] Confidence-based result limiting (hide very low confidence results)

### AC3: Clear Mapping Explanations with Citations âœ…
- [ ] Each mapping includes reasoning explanation
- [ ] Relevant CWE description snippets quoted
- [ ] Clear connection between query and CWE recommendation
- [ ] Source citations for all quoted material

### AC4: CWE Relationship Exploration âœ…
- [ ] Users can ask about CWE relationships during conversation
- [ ] Parent/child relationship navigation works
- [ ] Related CWE suggestions provided automatically
- [ ] Relationship chaining maintains context

### AC5: Low-Confidence Query Improvement Guidance âœ…
- [ ] Low confidence triggers specific improvement suggestions
- [ ] Guidance includes examples of better queries
- [ ] Persona-specific improvement recommendations
- [ ] Clear indicators when results may be incomplete

## Verification Steps

### Functional Testing
1. **Confidence Score Validation**
   ```bash
   # Test query with clear CWE match (should be high confidence)
   poetry run python -c "
   from apps.chatbot.src.query_handler import CWEQueryHandler
   handler = CWEQueryHandler(database_url, api_key)
   results = handler.process_query('What is CWE-79?', {'persona': 'Developer'})
   assert results[0]['confidence'] > 0.8
   "
   ```

2. **Explanation Quality Testing**
   - Submit queries and verify explanations include CWE snippets
   - Check that reasoning clearly connects query to recommendations
   - Ensure all mappings include source citations

3. **Relationship Navigation Testing**
   ```bash
   # Test relationship queries
   "Tell me about CWE-79"  # Initial query
   "What are related CWEs?" # Follow-up should show relationships
   "Show me parent CWEs"   # Should navigate hierarchy
   ```

4. **Filtering Validation**
   ```bash
   # Verify prohibited CWEs are filtered
   # Check that deprecated/discouraged CWEs don't appear
   # Confirm maximum 5 results limit
   ```

### Integration Testing
- Run existing E2E test suite to ensure no regressions
- Add new tests for confidence scoring workflows
- Test relationship exploration in Chainlit UI
- Verify persona-specific explanation differences

### Security Validation
- Ensure no sensitive information in explanations
- Validate input sanitization for relationship queries
- Check that explanation generation is safe from injection

### Performance Checks
- Verify response times remain under 5 seconds with explanations
- Check confidence calculation doesn't add significant overhead
- Ensure relationship queries perform efficiently

## Time Estimation

- **Phase 1 (Core Processing)**: 4 hours
- **Phase 2 (Relationships & Suggestions)**: 3 hours
- **Phase 3 (Query Handler Integration)**: 4 hours
- **Phase 4 (UI Rendering & Actions)**: 3 hours
- **Phase 5 (Testing & Validation)**: 2 hours
- **Total Estimated Time**: 16 hours
- **Buffer Time**: 2 hours for integration issues
- **Total with Buffer**: 18 hours

**Critical Path**: Phase 1 â†’ Phase 3 â†’ Phase 4 (linear dependency)
**Parallel Work**: Phase 2 can be developed alongside Phase 1
**Dependencies**: None - all required infrastructure from Story 2.1 is complete

## Risk Mitigation

### Technical Risks
1. **Risk**: Confidence scoring may not align with user expectations
   - **Mitigation**: Use existing hybrid scores as baseline, validate with test queries
   - **Fallback**: Adjust confidence thresholds based on testing results

2. **Risk**: Explanation generation may impact performance
   - **Mitigation**: Build explanations from existing retrieved chunks, no additional queries
   - **Fallback**: Cache common explanations, simplify if needed

3. **Risk**: CWE relationship data may be incomplete in corpus
   - **Mitigation**: Use existing relationship sections from ingestion, validate data availability
   - **Fallback**: Start with basic relationships, enhance iteratively

### Dependency Risks
- **Low Risk**: All dependencies (Story 2.1 components) are production-ready
- **Mitigation**: Component interfaces are stable and well-tested

### Timeline Risks
- **Risk**: Integration complexity may exceed estimates
- **Mitigation**: 20% buffer time included, phased approach allows early testing
- **Fallback**: Prioritize core confidence scoring if time becomes constrained

## Next Steps

After Story 3.2 completion:

1. **Story 3.3**: User Feedback Integration
   - Build on confidence scoring for feedback collection
   - Use explanation quality metrics for improvement tracking

2. **Performance Optimization**
   - Monitor confidence calculation impact
   - Optimize explanation generation if needed

3. **Enhanced Relationship Features**
   - Add visual relationship mapping
   - Implement relationship-based recommendations

4. **Documentation Updates**
   - Update API documentation with confidence scoring
   - Create user guide for relationship exploration
   - Document explanation quality standards

## Implementation Notes

### Leveraging Existing Infrastructure
- **Query Handler**: Already provides scored results - we enhance scoring interpretation
- **Response Generator**: Already builds context - we enhance with explanations and confidence
- **Vector Database**: Already contains relationship data - we add structured access
- **Testing Framework**: Already comprehensive - we extend with new test cases

### Minimal New Code Strategy
- Confidence scoring uses existing hybrid scores as input
- Explanations built from existing retrieved chunks
- Relationship data extracted from existing corpus sections
- New modules integrate cleanly with existing architecture

This approach ensures rapid implementation while building on proven, production-ready components from Story 2.1.