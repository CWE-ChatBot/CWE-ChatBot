Great foundation—nice guardrails and clear error paths. Here’s a focused review with “do-first” changes, small refactors, and assurance ideas. I’ll keep this tight and practical.

High-impact fixes (worth doing now)

Deployment entry-point mismatch (bug)

Your deploy command uses --entry-point=function_entry but the code defines pdf_worker.

Fix: either change deploy flag to --entry-point=pdf_worker or add:

# alias for Cloud Functions entry point
function_entry = pdf_worker


Hash & structured logging (traceable, no PII)

Log a SHA-256 of the input and sizes to correlate incidents without content.

import hashlib
pdf_sha256 = hashlib.sha256(pdf_data).hexdigest()
logger.info(json.dumps({"event":"ingest", "sha256": pdf_sha256,
                        "bytes": len(pdf_data), "pages": page_count}))


Keep logs JSON-structured to enable Cloud Logging filters/SIEM.

Tighten pdfminer invocation

extract_text can be slow; disable caching and respect explicit page bounds.

from pdfminer.high_level import extract_text_to_fp
from pdfminer.layout import LAParams

def extract_pdf_text(pdf_data: bytes, max_pages: int = MAX_PAGES) -> str:
    if not HAS_PDFMINER:
        raise ImportError("pdfminer.six not available")
    output = io.StringIO()
    laparams = LAParams()
    # process at most MAX_PAGES explicitly
    extract_text_to_fp(io.BytesIO(pdf_data), output,
                       laparams=laparams, caching=False,
                       maxpages=max_pages)
    text = output.getvalue()
    if len(text) > MAX_OUTPUT_CHARS:
        text = text[:MAX_OUTPUT_CHARS] + "\n\n[Content truncated at 1,000,000 characters]"
    return text


(You already gate at 50 pages; this makes the extractor obey it.)

Fail-closed Content-Type variants

You split on ;, good. Add common alias application/octet-stream rejection explicitly (some clients try this). You already reject by exact match which is fine; just keep it intentional in code comments and docs so clients know to send application/pdf.

Security headers for API responses

Add Cache-Control: no-store to avoid intermediaries caching potentially sensitive text. (X-Frame-Options isn’t meaningful for JSON APIs but harmless.)

headers.update({"Cache-Control": "no-store"})

Security hardening (defense-in-depth)

Zip/stream bombs & object count limits

PDFs can hide huge compressed streams. pikepdf/qpdf mitigates some risks but not all DoS patterns.

Mitigation options:

Open & sanitize in a separate process with a hard memory/CPU limit (e.g., run a tiny worker via subprocess or Cloud Run sidecar with container memory limit). If the child dies, return 422 pdf_sanitization_failed.

If staying in-process, add Cloud Functions memory (>=512Mi) and keep timeout conservative (you set 60s; good). Consider reducing to 30s if SLOs allow.

Model Armor availability: “fail-open” vs “fail-closed”

You fail-closed if enabled but unavailable—good for safety. Consider circuit-breaking:

If N consecutive failures in a window, log critical with sha256 and emit 503 pdf_content_scanner_unavailable so callers can retry/back-off distinctly from 422 pdf_content_blocked.

Bound request body earlier

In GCF HTTP you’ll still get the full body, but document 10 MB limit at the load balancer and enforce via Cloud Armor rules to reject oversized requests before the function.

WAF & rate-limit

Use Cloud Armor to throttle by IP/service account and require JWT audience == function URL if you ever front with an HTTPS LB. You already have internal-only ingress + IAM—great. Keep it.

CORS (only if browsers will call it)

If a web app might hit this directly, set minimal CORS (Access-Control-Allow-Origin: <your origin>) for POST only. Otherwise skip (internal-only).

Simplicity & refactors

Single open of the PDF

You open once to count pages and again to sanitize. You can sanitize first then count pages on the sanitized bytes to avoid double parse:

# after magic/size check
sanitized_data = sanitize_pdf(pdf_data)     # opens once
page_count = count_pdf_pages(sanitized_data)
if page_count > MAX_PAGES:
    return (... "pdf_too_many_pages" ...)
text = extract_pdf_text(sanitized_data, max_pages=MAX_PAGES)


Trade-off: you do a bit more work on a too-many-pages PDF, but you drop one open overall and ensure counting is on sanitized input.

Consistent error schema

Today some errors return only "error", others include "message". Consider a stable shape:

{ "ok": false, "error": { "code": "pdf_content_blocked", "message": "…" } }


And success:

{ "ok": true, "data": {...} }


Config dataclass

Gather env/config in a small dataclass; it aids testing and avoids global state creep.

Name things by behavior

sanitize_text_with_model_armor → guard_text_with_model_armor (reads cleaner that it may block, not just mutate).

Return only what callers need

If downstream never needs the full text (e.g., it sends directly to an LLM), consider returning a handle (hash + GCS object) instead of the text body. If you stick with no-persistence, keep returning text but add gzip compression on the LB.

Robustness & assurance practices

Golden & “evil” corpus tests

Keep a small corpus in CI: normal PDFs, encrypted PDF (expect pdf_encrypted), JS-embedded, XFA, embedded files, malformed xref, super-compressed stream. Assert:

correct error codes

size/page gates

sanitization removes /OpenAction, /AA, /JavaScript, /Names.EmbeddedFiles, /AcroForm.XFA

model-armor paths (enabled/disabled) behave.

Fuzzing (budget-friendly)

Nightly AFL/pdfcrack/mutational fuzz via container against a tiny CLI wrapper for sanitize_pdf (not the HTTP path). Even a 30–60 minute run catches surprising parser edges.

Observability

Emit latency buckets (sanitize_ms, extract_ms), outcome counts (allowed/blocked/encrypted/magic_fail) to Cloud Monitoring. Useful for alerting when input mix changes or a new DoS vector appears.

Supply-chain

Pin wheels:

pikepdf (backed by qpdf—watch for security advisories),

pdfminer.six (keep updated; known perf quirks),

google-cloud-modelarmor.

Use GCP Artifact Registry with VEX/SLSA where feasible.

Threat model doc (1 page)

Note trust boundaries (Function, Model Armor, IAM), assets (PDF bytes, extracted text), and decisions (fail-closed on scanner, no disk). This pays off during audits.

Tiny nits

X-Frame-Options: DENY isn’t harmful but is irrelevant for JSON APIs; consider dropping to keep headers minimal.

MAX_OUTPUT_CHARS truncation message states 1,000,000—keep the number dynamic from the constant to avoid drift.

Consider accepting %PDF- within the first 1 KB (some producers add a short BOM/whitespace). Spec usually puts it first, but this makes you slightly more tolerant:

if b"%PDF-" not in pdf_data[:1024]:
    ...

Example diff snippets

Add no-store and fix entry-point

headers = {
    "Content-Type": "application/json",
    "X-Content-Type-Options": "nosniff",
    "Referrer-Policy": "no-referrer",
    "X-Frame-Options": "DENY",
    "Cache-Control": "no-store",
}

function_entry = pdf_worker


Add sha256 logging

import hashlib
...
pdf_data = request.get_data()
pdf_sha256 = hashlib.sha256(pdf_data).hexdigest()
...
logger.info(json.dumps({"event":"request_received","sha256":pdf_sha256,"bytes":len(pdf_data)}))


Harden extractor

text = extract_pdf_text(sanitized_data, max_pages=min(page_count, MAX_PAGES))

Bottom line

Fix the entry-point, add no-store, hash-based structured logs, and bounded pdfminer—those are quick wins.

If you can spare a little time, sanitize once → count → extract, add Cloud Armor size/rate limits, and introduce latency/outcome metrics.

Longer-term, consider a process-isolated sanitizer for DoS resilience and keep a small “evil PDF” CI corpus.

Overall, this is already clean, memory-only, and IAM-protected—nice work.